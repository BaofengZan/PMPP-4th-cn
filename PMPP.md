## 1.4 并行编程的挑战

并行编程为何困难？有人曾经说过，如果你不关心性能，那么并行编程就非常简单。你几乎可以在一个小时内写出一个并行程序。但是，如果你不关心性能，为什么要写一个并行程序呢？这本书将讨论在实现高性能并行编程时遇到的几个挑战。首先，设计具有与顺序算法相同算法（计算）复杂度的并行算法可能具有挑战性。许多并行算法执行的工作量与它们的顺序对应物相同。然而，一些并行算法做的工作比它们的顺序对应物更多。实际上，有时它们可能做了这么多额外的工作，以至于对于大型输入数据集来说，它们的运行速度最终会变慢。

这尤其是一个问题，因为快速处理大型输入数据集是并行编程的一个重要动机。例如，许多现实世界的问题最自然地用数学递归来描述。并行化这些问题通常需要非直观的方式来思考问题，并且可能需要在执行期间进行冗余工作。有一些重要的算法原语，如前缀和，可以促进将问题的顺序递归表述转换为更并行的形式。我们将在第11章“前缀和（扫描）”中更正式地介绍工作效率的概念，并说明设计并行算法时所涉及的方法和权衡，这些算法与它们的顺序对应物具有相同的计算复杂度。

其次，许多应用程序的执行速度受到内存访问延迟和/或吞吐量的限制。我们称这些应用程序为内存绑定；相比之下，计算绑定应用程序受到每字节数据执行的指令数量的限制。实现内存绑定应用程序的高性能并行执行通常需要提高内存访问速度的方法。我们将在第5章“内存架构和数据局部性”和第6章“性能考虑”中介绍内存访问优化技术，并将这些技术应用于几章中的并行模式和应用程序。

第三，与顺序对应物相比，并行程序的执行速度通常对输入数据特征更敏感。许多现实世界的应用程序需要处理具有广泛变化特征的输入，例如不稳定或不可预测的数据大小和不均匀的数据分布。这些大小和分布的变化可能导致并行线程被分配不均的工作量，并且可以显著降低并行执行的有效性。并行程序的性能有时可能因这些特征而发生巨大变化。我们将在介绍并行模式和应用程序的章节中介绍规范化数据分布和/或动态细化线程数量的技术，以解决这些挑战。

第四，一些应用程序可以在不需要跨不同线程的大量协作的情况下进行并行化。这些应用程序通常被称为“尴尬地并行”。其他应用程序需要线程相互协作，这需要使用同步操作，如屏障或原子操作。这些同步操作给应用程序带来了开销，因为线程经常发现自己在等待其他线程而不是执行有用的工作。我们将在整本书中讨论减少这种同步开销的各种策略。幸运的是，研究人员已经解决了大多数这些挑战。此外，跨应用领域有共同的模式，允许我们将在一个领域中派生出的解决方案应用于其他领域的挑战。这就是为什么我们将在重要并行计算模式和应用程序的上下文中介绍解决这些挑战的关键技术的主要原因。

## 1.5 相关的并行编程接口

在过去的几十年中，已经提出了许多并行编程语言和模型。其中最广泛使用的是OpenMP（开放，2005）用于共享内存多处理器系统，以及消息传递接口（MPI）（MPI，2009）用于可扩展的集群计算。两者都已成为由主要计算机供应商支持的标准编程接口。OpenMP的实现包括编译器和运行时系统。程序员向OpenMP编译器指定有关循环的指令（命令）和提示（pragmas）。有了这些指令和提示，OpenMP编译器生成并行代码。运行时系统通过管理并行线程和资源来支持并行代码的执行。OpenMP最初是为CPU执行设计的，并且已经扩展以支持GPU执行。OpenMP的主要优点是它提供了编译器自动化和运行时支持，以抽象出程序员的许多并行编程细节。这种自动化和抽象可以帮助使应用程序代码在不同供应商生产的系统之间以及同一供应商的不同代系统之间更具可移植性。我们称这种属性为性能可移植性。然而，有效的OpenMP编程仍然需要程序员理解所有涉及的详细并行编程概念。因为CUDA为程序员提供了这些并行编程细节的显式控制，所以它是一个极好的学习工具，即使对于那些希望将OpenMP作为他们主要编程接口的人也是如此。此外，根据我们的经验，OpenMP编译器仍在不断发展和改进。许多程序员可能需要在OpenMP编译器不足的领域使用类似CUDA的接口。另一方面，MPI是一个计算节点在集群中不共享内存的编程接口（MPI，2009）。所有数据共享和交互必须通过显式消息传递完成。MPI已在高性能计算（HPC）中得到广泛应用。用MPI编写的应用程序已成功在拥有超过100,000个节点的集群计算系统上运行。今天，许多HPC集群采用异构CPU/GPU节点。将应用程序移植到MPI所需的工作量可能相当高，因为计算节点之间缺乏共享内存。程序员需要执行域分解，将输入和输出数据分配到各个节点。基于域分解，程序员还需要调用消息发送和接收函数来管理节点之间的数据交换。相比之下，CUDA为GPU中的并行执行提供了共享内存，以解决这一困难。虽然CUDA是每个节点的有效接口，但大多数应用程序开发人员需要在集群级别使用MPI进行编程。此外，通过API（如NVIDIA集体通信库（NCCL））对CUDA的多GPU编程支持也在增加。因此，了解如何在现代计算集群中使用多GPU节点进行联合MPI/CUDA编程对于高性能计算领域的并行程序员来说很重要，这是第20章“编程异构计算集群”中介绍的主题。2009年，包括苹果、英特尔、AMD/ATI和NVIDIA在内的几个主要行业参与者联合开发了一个名为Open Compute Language（OpenCL）（The Khronos Group，2009）的标准化编程模型。与CUDA类似，OpenCL编程模型定义了语言扩展和运行时API，允许程序员管理大规模并行处理器中的并行性和数据传递。与CUDA相比，OpenCL更多地依赖API而不是语言扩展。这允许供应商快速适应他们现有的编译器和工具来处理OpenCL程序。OpenCL是一个标准化的编程模型，因为用OpenCL开发的应用可以正确运行，无需修改，就可以在所有支持OpenCL语言扩展和API的处理器上运行。然而，为了在新处理器上实现高性能，可能需要修改应用程序。熟悉OpenCL和CUDA的人都知道OpenCL和CUDA的关键概念和特性之间有着显著的相似性。也就是说，一个CUDA程序员可以以最小的努力学习OpenCL编程。更重要的是，几乎所有在使用CUDA中学到的技术都可以轻松地应用到OpenCL编程中。

## 1.6 总体目标

我们的主要目标是教会您，读者，如何编程大规模并行处理器以实现高性能。因此，本书的大部分内容都致力于开发高性能并行代码的技术。我们的方法不需要大量的硬件专业知识。然而，您需要对并行硬件架构有良好的概念理解，以便能够理解您的代码的性能行为。因此，我们将专门用一些篇幅来直观地理解硬件架构的基本特征，并用很多篇幅来介绍开发高性能并行程序的技术。特别是，我们将专注于计算思维（Wing，2006）技术，这将使您能够以适合在大规模并行处理器上高效执行的方式来思考问题。在大多数处理器上进行高性能并行编程需要一些硬件工作原理的知识。构建允许程序员在没有这种知识的情况下开发高性能代码的工具和机器可能需要很多年。即使我们拥有这样的工具，我们怀疑了解硬件的程序员将能够比那些不了解的人更有效地使用这些工具。因此，我们专门用第4章“计算架构和调度”来介绍GPU架构的基础知识。我们还将讨论更专业的架构概念，作为我们讨论高性能并行编程技术的一部分。

我们的第二个目标是教授并行编程以实现正确的功能和可靠性，这在并行计算中是一个微妙的问题。过去在并行系统上工作过的程序员知道，仅仅实现初始性能是不够的。挑战在于以一种方式实现它，以便您可以调试代码并支持用户。CUDA编程模型鼓励使用简单的屏障同步、内存一致性和原子性来管理并行性。此外，它提供了强大的工具，允许人们不仅调试功能的方面，还可以调试性能瓶颈。我们将展示通过专注于数据并行性，可以在应用程序中实现高性能和高可靠性。

我们的第三个目标是通过探索并行编程的方法来实现跨未来硬件代的可扩展性，以便未来的机器，将越来越并行，可以使您的代码比今天的机器运行得更快。我们希望帮助您掌握并行编程，使您的程序能够扩展到新一代机器的性能水平。这种可扩展性的关键是通过规范化和局部化内存数据访问来最小化对关键资源的消耗和数据结构更新中的冲突。因此，开发高性能并行代码的技术对于确保应用程序未来的可扩展性也很重要。实现这些目标需要大量的技术知识，因此，我们将在本书中介绍许多并行编程的原理和模式（Mattson等人，2004）。我们不会单独教授这些原理和模式。我们将在并行化有用应用程序的上下文中教授它们。然而，我们无法涵盖所有内容，因此我们已经选择了最有用和经过验证的技术来进行详细讨论。事实上，当前版本在并行模式方面有显著增加的章节数量。

我们已经准备好为您提供本书其余部分的快速概述。

1.7 书籍组织结构

本书分为四个部分。第一部分涵盖了并行编程的基本概念、数据并行性、GPU以及性能优化。这些基础章节为读者提供了成为GPU程序员所需的基本知识和技能。第二部分涵盖了原始并行模式，第三部分涵盖了更高级的并行模式和应用。这两部分应用了第一部分中学到的知识和技能，并根据需要引入了其他GPU架构特性和优化技术。最后一个部分，第四部分，介绍了高级实践，以完成希望成为专家级GPU程序员的读者的知识。

第1部分关于基本概念，包括第2至6章。第2章《异构数据并行计算》介绍了数据并行性和CUDA C编程。本章基于读者具有C编程的先验经验。它首先介绍了CUDA C作为C的简单、小型扩展，支持异构CPU/GPU计算和广泛使用的单程序、多数据并行编程模型。然后，它涵盖了涉及以下内容的思维过程：(1) 确定应用程序中要并行化的部分；(2) 隔离要由并行化代码使用的数据，使用API函数在并行计算设备上分配内存；(3) 使用API函数将数据传输到并行计算设备；(4) 将并行部分开发成将由并行线程执行的内核函数；(5) 启动内核函数以供并行线程执行；(6) 最终使用API函数调用将数据传回主机处理器。我们使用向量加法的运行示例来说明这些概念。虽然第2章的目标是教授足够的CUDA C编程模型概念，以便读者可以编写一个简单的并行CUDA C程序，但它涵盖了开发任何并行编程接口的并行应用程序所需的几个基本技能。第3章《多维网格和数据》介绍了CUDA并行执行模型的更多细节，特别是它与使用多维线程组织处理多维数据的关系。它提供了足够的洞察力，以创建、组织、绑定资源和绑定数据的线程，使读者能够使用CUDA C实现复杂的计算。第4章《计算架构和调度》介绍了GPU架构，重点是计算核心的组织方式以及线程如何在这些核心上执行的调度。讨论了各种架构考虑因素，以及它们对在GPU架构上执行的代码性能的影响。这些包括透明可扩展性、SIMD执行和控制分歧、多线程和延迟容忍度以及占用率等概念，这些都在本章中定义和讨论。第5章《内存架构和数据局部性》通过讨论GPU的内存架构，扩展了第4章《计算架构和调度》的内容。它还讨论了可用于持有CUDA变量以管理数据传递和提高程序执行速度的特殊内存。我们介绍了分配和使用这些内存的CUDA语言特性。适当使用这些内存可以大幅提高数据访问吞吐量，并有助于缓解内存系统中的流量拥堵。第6章《性能考虑》介绍了当前CUDA硬件的几个重要性能考虑因素。特别是，它更详细地介绍了线程执行和内存访问的期望模式。这些细节为程序员提供了概念基础，以便他们能够推理他们关于组织计算和数据的决策的后果。本章最后给出了一个常见优化策略的清单，GPU程序员经常使用这些策略来优化任何计算模式。这份清单将在整个书的后两部分中用来优化各种并行模式和应用。

第二部分关于原始并行模式，包括第7至12章。第7章《卷积》介绍了卷积，这是一种经常使用的并行计算模式，它源于数字信号处理和计算机视觉，并需要仔细管理数据访问局部性。我们还使用这种模式来介绍现代GPU中的常量内存和缓存。第8章《模板》介绍了模板模式，它与卷积类似，但源于解决微分方程，并具有特定的特征，为进一步优化数据访问局部性提供了独特的机会。我们还使用这种模式来介绍线程和数据的三维组织，并展示第6章《性能考虑》中引入的针对线程粒度的优化。第9章《并行直方图》涵盖了直方图，这是一种在统计数据分析以及大型数据集的模式识别中广泛使用的模式。我们还使用这种模式来介绍原子操作作为协调对共享数据并发更新的手段，以及减少这些操作开销的私有化优化。第10章《归约和最小化分歧》介绍了归约树模式，它用于总结一组输入数据。我们还使用这种模式来演示控制分歧对性能的影响，并展示如何减轻这种影响的技术。第11章《前缀和（扫描）》介绍了前缀和，或扫描，这是一个重要的并行计算模式，它将本质上的顺序计算转换为并行计算。我们还使用这种模式来介绍并行算法中工作效率的概念。最后，第12章《合并》涵盖了并行合并，这是一种在分而治之的工作划分策略中广泛使用的模式。我们还使用本章来介绍动态输入数据识别和组织。

第三部分关于高级并行模式和应用，与第二部分在精神上相似，但涵盖的模式更复杂，通常包括更多的应用上下文。因此这些章节不太侧重于介绍新技术或特性，而更侧重于特定于应用的考虑。对于每个应用，我们首先确定并行执行基本结构的不同表述方式，并讨论每种替代方案的优缺点。然后，我们将经历实现高性能所需的代码转换步骤。这些章节帮助读者将前面章节的所有材料整合在一起，并在他们承担自己的应用开发项目时为他们提供支持。第三部分包括第13至19章。第13章《排序》介绍了两种形式的并行排序：基数排序和归并排序。这种高级模式利用了前面章节中介绍的更原始的模式，特别是前缀和和并行合并。第14章《稀疏矩阵计算》介绍了稀疏矩阵计算，它广泛用于处理非常大的数据集。

第15章《图遍历》介绍了图算法以及如何在GPU编程中高效实现图搜索。讨论了多种并行化图算法的不同策略，并讨论了图结构对选择最佳算法的影响。这些策略建立在更原始的模式上，如直方图和合并。第16章《深度学习》涵盖了深度学习，这是GPU计算中变得越来越重要的一个领域。我们介绍了卷积神经网络的有效实现，并将在其他来源中进行更深入的讨论。卷积神经网络的有效实现利用了诸如平铺和卷积等模式的技术。第17章《迭代磁共振成像重建》涵盖了非笛卡尔MRI重建以及如何利用循环融合和分散-聚集转换等技术来增强并行性和减少同步开销。第18章《静电势图》涵盖了分子可视化和分析，这些技术从稀疏矩阵计算中受益，通过应用不规则数据的处理技术。第19章《并行编程和计算思维》介绍了计算思维，这是一种以更易于高性能计算的方式制定和解决计算问题的艺术。它通过涵盖组织程序的计算任务的概念，使它们可以并行完成。我们首先讨论将抽象科学、问题特定的概念转化为计算任务的转换过程，这是生产高质量应用软件，无论是串行还是并行的重要第一步。本章还讨论了并行算法结构及其对应用程序性能的影响，这基于CUDA的性能调整经验。虽然我们不深入讨论这些替代并行编程风格的实现细节，但我们期望读者能够通过在本书中获得的基础学会使用任何一种编程风格。我们还提供了一个高层次的案例研究，以展示通过创造性计算思维可以看到的机会。

第四部分关于高级实践，包括第20至22章。第20章《编程异构计算集群》涵盖了在异构集群上的CUDA编程，其中每个计算节点由CPU和GPU组成。我们讨论了与CUDA一起使用MPI来整合节点间计算和节点内计算以及由此产生的通信问题和实践。第21章《CUDA动态并行性》涵盖了动态并行性，这是GPU根据数据或程序结构动态为自己创建工作的能力，而不是总是等待CPU这样做。第22章《高级实践和未来发展》介绍了CUDA程序员需要了解的一些杂项高级功能和实践的列表。这些包括零拷贝内存、统一虚拟内存、多个内核的同时执行、函数调用、异常处理、调试、分析、双精度支持、可配置的缓存/暂存区大小等主题。例如，CUDA的早期版本在CPU和GPU之间提供了有限的共享内存能力。程序员需要显式管理CPU和GPU之间的数据传输。然而，当前版本的CUDA支持诸如统一虚拟内存和零拷贝内存等功能，实现CPU和GPU之间数据的无缝共享。有了这样的支持，CUDA程序员可以声明变量和数据结构为CPU和GPU共享。运行时硬件和软件根据需要自动维护一致性并执行优化的数据传输操作。这种支持显著降低了在计算和I/O活动期间重叠数据传输所涉及的编程复杂性。在教科书的介绍部分，我们使用显式数据传输的API，以便读者更好地理解底层发生的事情。我们稍后在第22章《高级实践和未来发展》中介绍统一虚拟内存和零拷贝内存。尽管本书中的章节基于CUDA，但它们帮助读者建立了一般并行编程的基础。我们相信，当我们通过具体示例学习时，人类理解得最好。也就是说，我们必须首先在特定的编程模型的上下文中学习概念，这为我们在将知识推广到其他编程模型时提供了坚实的基础。随着我们这样做，我们可以借鉴我们从CUDA示例中获得的具体经验。深入的CUDA经验还使我们能够获得成熟度，这将帮助我们学习可能甚至不适用于CUDA模型的概念。

第23章，结论与展望，对大规模并行编程的未来进行了总结和展望。我们首先回顾我们的目标，并总结这些章节如何结合在一起以帮助实现目标。然后，我们最后预测，这些在大规模并行计算方面的快速发展将使其成为未来十年最令人兴奋的领域之一。

# 2 异构数据并行计算

 数据并行性是指在数据集的不同部分上所要执行的计算工作可以相互独立地完成，从而可以相互并行地完成的现象。许多应用程序显示出大量的数据并行性，这使它们易于进行可伸缩的并行执行。因此，对于并行程序员来说，熟悉数据并行性的概念和用来编写利用数据并行性的代码的并行编程语言构造是很重要的。在本章中，我们将使用CUDA C语言构造来开发一个简单的数据并行程序。

## 2.1 数据并行性

数据并行性指的是在不同数据集的部分上执行的计算工作可以相互独立，因此可以并行完成的现象。许多应用程序展现出丰富的数据并行性，使它们适合于可扩展的并行执行。因此，对于并行程序员来说，熟悉数据并行性的概念以及用于编写代码利用数据并行性的并行编程语言结构非常重要。在本章中，我们将使用CUDA C语言结构来开发一个简单的数据并行程序。

当现代软件应用程序运行缓慢时，问题通常是数据——需要处理的数据量太大。图像处理应用程序处理的图像或视频包含数百万到数万亿像素。科学应用程序使用数十亿个网格点来模拟流体动力学。分子动力学应用程序必须模拟数千到数十亿个原子之间的相互作用。航空公司的调度涉及数千次航班、机组人员和机场登机口。这些像素、粒子、网格点、相互作用、航班等通常可以大部分独立处理。例如，在图像处理中，将彩色像素转换为灰度只需要该像素的数据。模糊图像需要将每个像素的颜色与附近像素的颜色平均，只需要该像素小邻域内的数据。甚至看似全局操作的，例如找出图像中所有像素的平均亮度，也可以分解为许多可以独立执行的较小计算。这种不同数据片段的独立评估是数据并行性的基础。编写数据并行代码需要（重新）组织计算，使其围绕数据进行，以便我们可以并行执行生成的独立计算，以更快地完成整个工作——通常要快得多。让我们通过一个将颜色转换为灰度的例子来说明数据并行性的概念。图2.1显示了一个由许多像素组成的彩色图像（左侧），每个像素包含红色、绿色和蓝色分数值（r, g, b），这些值从0（黑色）变化到1（完全强度）。要将彩色图像（图2.1左侧）转换为灰度图像（右侧），我们通过应用以下加权和公式计算每个像素的亮度值L：

\[ L = 0.21r + 0.72g + 0.07b \]

![image-20240718141219286](imgs/image-20240718141219286.png)



在RGB颜色表示中，图像中的每个像素都存储为(r, g, b)值的元组。图像的一行的格式是(r g b) (r g b) ... (r g b)，如图中所示。每个元组指定了红色(R)、绿色(G)和蓝色(B)的混合。也就是说，对于每个像素，r、g和b值表示在渲染像素时红色、绿色和蓝色光源的强度（0表示暗，1表示完全强度）。

![image-20240718141332091](imgs/image-20240718141332091.png)

实际上，这三种颜色的允许混合在行业指定的颜色空间中有所不同。在这里，AdobeRGB颜色空间中三种颜色的有效组合显示为三角形的内部。每个混合的垂直坐标（y值）和水平坐标（x值）显示了像素强度的分数，应该分配给G和R。剩余的分数（1-y-x）的像素强度应该分配给B。要渲染图像，每个像素的r、g、b值用于计算像素的总强度（亮度）以及混合系数（x，y，1-y-x)。

如果我们考虑输入是一个以RGB值数组I组织的图像，输出是一个相应的亮度值数组O，我们得到图2.2所示的简单计算结构。例如，O[0]是通过根据上述公式计算I[0]中RGB值的加权和生成的；O[1]是通过计算I[1]中RGB值的加权和生成的；O[2]是通过计算I[2]中RGB值的加权和生成的；以此类推。这些每个像素的计算彼此独立。显然，颜色到灰度的转换展现出丰富的数据并行性。当然，完整应用程序中的数据并行性可能更复杂，本书的大部分内容都致力于教授寻找和利用数据并行性的并行思维。

![image-20240718142806308](imgs/image-20240718142806308.png)

任务并行性与数据并行性 

并行性不是并行编程中使用的唯一类型。任务并行性也在并行编程中得到了广泛的应用。任务并行性通常通过应用程序的任务分解来暴露。例如，一个简单的应用程序可能需要执行向量加法和矩阵-向量乘法。如果这两个任务可以独立完成，那么就存在任务并行性。I/O和数据传输也是任务的常见来源。在大型应用程序中，通常有更多独立的任务，因此任务并行性的数量也更大。例如，在分子动力学模拟器中，自然任务列表包括振动力、旋转力、非键合力的邻近识别、非键合力、基于速度和位置的速度和位置等其他物理属性。通常，数据并行性是并行程序可扩展性的主要来源。对于大型数据集，通常可以找到丰富的数据并行性，以利用大规模并行处理器，并允许应用程序性能随着每一代硬件拥有更多的执行资源而增长。尽管如此，任务并行性在实现性能目标中也可以发挥重要作用。我们将在稍后介绍流时涵盖任务并行性。

## 2.2 CUDA C程序结构

我们现在已经准备好学习如何编写一个CUDA C程序来利用数据并行性来加速执行。CUDA C1扩展了流行的ANSI C编程语言，增加了最小的新语法和库函数，让程序员可以针对包含CPU核心和大规模并行GPU的异构计算系统进行编程。顾名思义，CUDA C是建立在NVIDIA的CUDA平台之上的。CUDA目前是大规模并行计算最成熟的框架。它在高性能计算行业中得到了广泛应用，拥有编译器、调试器和分析器等基本工具，这些工具在最常见的操作系统上都可用。CUDA C程序的结构反映了计算机中主机（CPU）和一台或多台设备（GPU）的共存。每个CUDA C源文件可以包含主机代码和设备代码的混合。默认情况下，任何传统的C程序都是只包含主机代码的CUDA程序。你可以在任何源文件中添加设备代码。设备代码用特殊的CUDA C关键字清楚地标记。设备代码包括函数，或者称为内核，其代码以数据并行的方式执行。CUDA程序的执行如图2.3所示。执行从主机代码（CPU串行代码）开始。当调用内核函数时，在设备上启动大量线程来执行内核。由内核调用启动的所有线程统称为网格。这些线程是CUDA平台上并行执行的主要工具。图2.3显示了两个网格线程的执行。我们很快将讨论这些网格是如何组织的。当网格的所有线程完成执行时，网格终止，执行继续在主机上进行，直到启动另一个网格。

![image-20240718143534047](imgs/image-20240718143534047.png)

注意图2.3显示了一个简化模型，其中CPU执行和GPU执行不重叠。许多异构计算应用程序管理重叠的CPU和GPU执行，以利用两者的优势。启动网格通常会产生许多线程来利用数据并行性。在颜色到灰度转换的例子中，每个线程可以用来计算输出数组O的一个像素。在这种情况下，应该由网格启动生成的线程数量等于图像中的像素数量。对于大图像，将生成大量线程。CUDA程序员可以假设这些线程的生成和调度只需要很少的时钟周期，这得益于高效的硬件支持。这与通常需要数千个时钟周期来生成和调度的传统CPU线程形成对比。在下一章中，我们将展示如何实现颜色到灰度转换和图像模糊内核。在本章的其余部分，我们将使用向量加法作为运行示例，以简化说明。

线程是现代计算机中处理器执行顺序程序的简化视图。一个线程由程序的代码、正在执行的代码点以及其变量和数据结构的值组成。就用户而言，线程的执行是顺序的。可以使用源级调试器通过一次执行一个语句、查看下一个将被执行的语句以及检查变量和数据结构的值来监控线程的进度。多年来，线程在编程中一直被使用。如果程序员想要在应用程序中开始并行执行，他/她将使用线程库或特殊语言创建和管理多个线程。在CUDA中，每个线程的执行也是顺序的。CUDA程序通过调用内核函数来启动并行执行，这会导致底层运行时机制在设备上启动一个线程网格，这些线程并行处理不同的数据部分。

## 2.3 向量加法内核

我们使用向量加法来演示CUDA C程序结构。向量加法可以说是最简单的可能的数据并行计算——与顺序编程中的“Hello World”等价。在我们展示向量加法的内核代码之前，先回顾一下传统向量加法（主机代码）函数的工作原理是很有帮助的。图2.4显示了一个简单的传统C程序，由一个主函数和一个向量加法函数组成。在我们的示例中，每当需要区分主机和设备数据时，我们将使用主机的变量名后缀“_h”，将设备使用的变量名后缀“_d”，以提醒我们这些变量的预期用途。由于图2.4中只有主机代码，我们只看到带有“_h”后缀的变量。

C语言中的指针 

在图2.4中，函数参数A、B和C是指针。在C语言中，指针可以用来访问变量和数据结构。虽然可以用以下方式声明一个浮点变量V：float V; 但可以用以下方式声明一个指针变量P：float * P; 通过将V的地址赋给P的语句P = &V，我们使P“指向”V。*P成为V的同义词。例如，U = *P将V的值赋给U。另一个例子，\*P = 3将V的值更改为3。在C程序中，可以通过指向其0元素的指针来访问数组。例如，语句P = &(A[0])使P指向数组A的0元素。P[i]成为A[i]的同义词。实际上，数组名A本身就是指向其0元素的指针。在图2.4中，将数组名A作为第一个参数传递给函数调用vecAdd，使函数的第一个参数A_h指向A的0元素。因此，函数体中的A_h[i]可以用来访问主函数中数组A的A[i]。有关C中指针详细使用的易于理解的解释，请参阅Patt和Patel（Patt和Patel，2020）。

![image-20240718143748612](imgs/image-20240718143748612.png)

假设要加的向量存储在A和B数组中，这些数组在主程序中被分配和初始化。输出向量在C数组中，也在主程序中被分配。为了简洁，我们没有展示A、B和C是如何在主函数中分配或初始化的细节。这些数组的指针被传递给vecAdd函数，以及包含向量长度的变量N。注意，vecAdd函数的参数带有“_h”后缀，以强调它们由主机使用。这种命名约定在我们接下来引入设备代码时将很有帮助。

图2.4中的vecAdd函数使用for循环遍历向量元素。在第i次迭代中，输出元素C_h[i]接收A_h[i]和B_h[i]的和。向量长度参数n用于控制循环，以便迭代次数与向量长度相匹配。该函数通过指针A_h、B_h和C_h分别读取A和B的元素，并写入C的元素。当vecAdd函数返回时，主函数中的后续语句可以访问C的新内容。

执行向量加法的一个直接方法是修改vecAdd函数，将其计算移动到设备上。这种修改后的vecAdd函数的结构如图2.5所示。第1部分在设备（GPU）内存中为A、B和C向量分配空间，并将A和B向量从主机内存复制到设备内存。第2部分调用实际的向量加法内核，以在设备上启动线程网格来执行实际的向量加法。第3部分将和向量C从设备内存复制到主机内存，并从设备内存中释放三个数组。

![image-20240718144200018](imgs/image-20240718144200018.png)

请注意，修订后的vecAdd函数本质上是一个外包代理，它将输入数据发送到设备，激活设备上的计算，并从设备收集结果。代理以一种方式这样做，使得主程序甚至不需要知道向量加法实际上是在设备上完成的。在实践中，这种“透明”的外包模型可能非常低效，因为所有的数据来回复制。通常，我们会将大型和重要的数据结构保留在设备上，并简单地从主机代码调用设备函数。然而，目前我们将使用简化的透明模型来介绍基本的CUDA C程序结构。修订后的函数的详细信息以及组成内核函数的方式将是本章其余部分的主题。

## 2.4 设备全局内存和数据传输

在当前的CUDA系统中，设备通常是硬件卡，它们配备了自己的动态随机存取内存，称为设备全局内存，或简称全局内存。例如，NVIDIA Volta V100配备了16GB或32GB的全局内存。将其称为“全局”内存是为了将其与程序员也可以访问的其他类型的设备内存区分开来。CUDA内存模型和不同类型设备内存的详细信息将在第5章“内存架构和数据局部性”中讨论。

对于向量加法内核，在调用内核之前，程序员需要在设备全局内存中分配空间，并将数据从主机内存传输到设备全局内存中分配的空间。这对应于图2.5的第1部分。类似地，在设备执行后，程序员需要将结果数据从设备全局内存传输回主机内存，并释放不再需要的设备全局内存中分配的空间。这对应于图2.5的第3部分。CUDA运行时系统（通常在主机上运行）提供了应用程序编程接口（API）函数，代表程序员执行这些活动。从这一点开始，我们将简单地说数据从主机传输到设备，作为将数据从主机内存复制到设备全局内存的简写。反之亦然。

在图2.5中，vecAdd函数的第1部分和第3部分需要使用CUDA API函数为A、B和C分配设备全局内存；将A和B从主机传输到设备；在向量加法后将C从设备传输到主机；并释放A、B和C的设备全局内存。我们首先将解释内存分配和释放函数。

![image-20240718144713481](imgs/image-20240718144713481.png)

图2.6展示了两个用于分配和释放设备全局内存的API函数。可以从主机代码中调用cudaMalloc函数为对象分配一块设备全局内存。读者应该注意到cudaMalloc与标准C运行库中的malloc函数之间的显著相似性。这是有意为之的；CUDA C是C语言的最小扩展。CUDA C使用标准C运行库的malloc函数来管理主机内存，并增加了cudaMalloc作为C运行库的扩展。通过尽可能保持与原始C运行库的接口接近，CUDA C最小化了C程序员重新学习这些扩展的使用时间。

cudaMalloc函数的第一个参数是要设置为指向已分配对象的指针变量的地址。指针变量的地址应该强制转换为(void **)，因为该函数期望一个通用指针；内存分配函数是一个通用函数，不限制于任何特定类型的对象。这个参数允许cudaMalloc函数将已分配内存的地址写入提供的指针变量，无论其类型如何。调用内核的主机代码将这个指针值传递给需要访问已分配内存对象的内核。cudaMalloc函数的第二个参数是要分配的数据的大小，以字节为单位。这个第二个参数的使用与C malloc函数的大小参数一致。

我们现在使用以下简单的代码示例来说明cudaMalloc和cudaFree的使用：

![image-20240718145729736](imgs/image-20240718145729736.png)

这是图2.5示例的延续。为了清晰起见，我们用“_d”后缀表示指针变量指向设备全局内存中的对象。传递给cudaMalloc的第一个参数是指针A_d的地址（即&A_d），并将其强制转换为void指针。当cudaMalloc返回时，A_d将指向为A向量分配的设备全局内存区域。传递给cudaMalloc的第二个参数是将要分配的区域的大小。由于大小是以字节为单位，程序员需要在确定size的值时，从数组的元素数量转换为字节数量。例如，在为n个单精度浮点元素数组分配空间时，size的值将为n乘以单精度浮点数的大小，而在当今的计算机中，单精度浮点数是4个字节。因此，size的值将为n*4。计算完成后，调用cudaFree并传递指针A_d作为参数，以释放A向量从设备全局内存中分配的存储空间。注意，cudaFree不需要改变A_d的值；它只需要使用A_d的值将已分配的内存返回到可用池中。因此，只有A_d的值而不是地址被传递为参数。

A_d、B_d和C_d中的地址指向设备全局内存中的位置。这些地址在主机代码中不应该被解引用。它们应该在调用API函数和内核函数时使用。在主机代码中解引用设备全局内存指针可能会引发异常或其他类型的运行时错误。

读者应该用类似的B_d和C_d指针变量声明以及相应的cudaMalloc调用完成图2.5中的第1部分。此外，图2.5的第3部分可以完成B_d和C_d的cudaFree调用。

一旦主机代码为数据对象在设备全局内存中分配了空间，它就可以请求将数据从主机传输到设备。这可以通过调用一个CUDA API函数来完成。图2.7显示了这样的API函数，即cudaMemcpy。cudaMemcpy函数接受四个参数。第一个参数是将要复制的数据对象的目标位置的指针。第二个参数指向源位置。第三个参数指定要复制的字节数。第四个参数指示参与复制的内存类型：主机到主机、主机到设备、设备到主机和设备到设备。例如，内存复制函数可以用来将数据从一个设备全局内存位置复制到设备全局内存中的另一个位置。

![image-20240718150027569](imgs/image-20240718150027569.png)

vecAdd 函数调用 cudaMemcpy 函数将 A_h 和 B_h 向量从主机内存复制到设备内存中的 A_d 和 B_d，然后进行加法运算，并将 C_d 向量从设备内存复制到主机内存中的 C_h。假设 A_h、B_h、A_d、B_d 和 size 的值已经像我们之前讨论的那样设置好了，那么三个 cudaMemcpy 调用如下所示。两个符号常量 cudaMemcpyHostToDevice 和 cudaMemcpyDeviceToHost 是 CUDA 编程环境中的预定义常量。请注意，通过正确地排列源指针和目标指针，并使用适当的传输类型常量，同一个函数可以用于双向数据传输。

![image-20240718150124202](imgs/image-20240718150124202.png)

概括而言，图2.4中的主要程序调用了在主机上执行的vecAdd。vecAdd函数的概要如图2.5所示，它在设备全局内存中分配空间，请求数据传输，并调用执行实际向量加法的核心函数。我们称这种类型的主机代码为调用核心函数的存根。我们在图2.8中展示了vecAdd函数的一个更完整的版本。

![image-20240718150228829](imgs/image-20240718150228829.png)

与图2.5相比，图2.8中的vecAdd函数对于第1部分和第3部分是完整的。第1部分为A_d、B_d和C_d在设备全局内存中分配空间，并将A_h传输到A_d，将B_h传输到B_d。这是通过调用cudaMalloc和cudaMemcpy函数完成的。鼓励读者编写自己的函数调用，并使用适当的参数值，并将他们的代码与图2.8中显示的代码进行比较。第2部分调用核心函数，将在随后的小节中描述。第3部分将向量和数据从设备复制到主机，以便在主函数中可以使用这些值。这是通过调用cudaMemcpy函数实现的。然后通过调用cudaFree函数（见图2.9）释放设备全局内存中A_d、B_d和C_d的内存。

CUDA中的错误检查和处理

一般来说，对于程序来说，检查和处理错误是非常重要的。 CUDA API函数在提供请求服务时返回标志，这些标志表明是否发生了错误。大多数错误是由于调用中使用了不适当的参数值。 为了简洁，我们的示例中不会显示错误检查代码。例如，图2.9显示了一个对cudaMalloc的调用： cudaMalloc((void**) &A_d, size); 实际上，我们应该用代码包围这个调用，以测试错误条件并打印出错误消息，这样用户就可以意识到发生了错误。以下是这样一个检查代码的简单版本：

![image-20240718150615195](imgs/image-20240718150615195.png)

这种方式下，如果系统没有足够的设备内存，用户将被告知情况。这可以节省很多调试时间。可以定义一个C宏来使源代码中的错误检查代码更加简洁。

## 2.5 内核函数和线程

在 CUDA C 中，核心函数指定了在并行阶段所有线程要执行的代码。由于所有这些线程执行相同的代码，CUDA C 编程是众所周知的单程序多数据（SPMD）（Atallah, 1998）并行编程风格的一个实例，这是并行计算系统中流行的编程风格。当程序的主机代码调用一个核心函数时，CUDA 运行时系统启动了一个线程网格，这些线程被组织成两级层次结构。每个网格被组织成一个线程块的数组，我们将简称为“块”。所有网格的块大小相同；在当前系统中，每个块最多可以包含 1024 个线程。图 2.9 显示了一个示例，其中每个块由 256 个线程组成。每个线程由一个从标记有线程在块中索引号的框中延伸出来的卷曲箭头表示。

许多编程语言都有内置变量。这些变量具有特殊的含义和目的。这些变量的值通常由运行时系统预先初始化，并且在程序中通常是只读的。程序员应该避免为其他目的重新定义这些变量。'

当调用核心函数时，主机代码指定了每个线程块中的线程总数。同一个核心函数可以在主机代码的不同部分用不同数量的线程调用。对于给定的线程网格，块中的线程数可以在内置变量blockDim中获取。blockDim变量是一个结构体，包含三个无符号整数字段（x、y和z），这些字段帮助程序员将线程组织成一维、二维或三维数组。对于一维组织，只使用x字段。对于二维组织，使用x和y字段。对于三维结构，使用所有三个x、y和z字段。组织线程的维度选择通常反映数据的维度。这是有意义的，因为线程是为了并行处理数据而创建的，所以线程的组织反映数据的组织是很自然的。在图2.9中，每个线程块被组织成一维线程数组，因为数据是一维向量。blockDim.x变量的值表示每个块中的线程总数，图2.9中是256。通常建议，出于硬件效率的原因，每个线程块的每个维度的线程数应该是32的倍数。我们稍后将重新讨论这一点。

![image-20240718151014476](imgs/image-20240718151014476.png)

CUDA核心函数可以访问两个额外的内置变量（threadIdx和blockIdx），这些变量允许线程相互区分并确定每个线程要处理的数据区域。threadIdx变量为块内的每个线程提供了一个唯一的坐标。在图2.9中，由于我们使用的是一维线程组织，因此只使用threadIdx.x。图2.9中每个线程的小阴影框内显示了每个线程的threadIdx.x值。每个块中的第一个线程的threadIdx.x变量值为0，第二个线程的值为1，第三个线程的值为2，以此类推。

> CUDA线程的层次化组织
> 像CUDA线程一样，许多现实世界的系统也是层次化组织的。美国的电话系统是一个很好的例子。在最顶层，电话系统由“区”组成，每个区对应一个地理区域。同一区内的所有电话线都有相同的3位数字“区号”。有时一个电话区比一个城市还要大。例如，伊利诺伊州中部的许多县和城市都位于同一个电话区，共享相同的区号217。在一个区内，每部电话线都有一个七位数字的本地电话号码，这使得每个区最多可以有大约一千万号码。
> 我们可以将每部电话线想象成CUDA线程，区号作为blockIdx的值，七位数字的本地号码作为threadIdx的值。这种层次化组织允许系统拥有非常大量的电话线，同时保持了对同一区呼叫的“局部性”。也就是说，当拨打同一区内的电话线时，拨打者只需要拨打本地号码。只要我们大多数电话都在本地区内拨打，我们很少需要拨打区号。如果我们偶尔需要拨打其他区的电话线，我们拨打1和区号，然后是本地号码。（这就是为什么任何区内的本地号码都不应以1开头的原因。）CUDA线程的层次化组织也提供了一种局部性。我们将很快研究这种局部性。

blockIdx变量为块中的所有线程提供了一个共同的块坐标。在图2.9中，第一个块中的所有线程在它们的blockIdx.x变量中的值都是0，第二个线程块中的值为1，以此类推。使用电话系统的类比，我们可以将threadIdx.x视为本地电话号码，将blockIdx.x视为区号。两者结合在一起，为全国的每条电话线提供了一个唯一的电话号码。类似地，每个线程可以结合它的threadIdx和blockIdx值，为自己在整个网格中创建一个唯一的全局索引。

在图2.9中，一个唯一的全局索引i被计算为\[i = blockIdx.x \times blockDim.x + threadIdx.x\]。回想一下，在我们的示例中blockDim是256。块0中的线程的i值范围从0到255。块1中的线程的i值范围从256到511。块2中的线程的i值范围从512到767。也就是说，这三个块中的线程的i值形成了从0到767的连续覆盖。由于每个线程使用i来访问A、B和C，这些线程覆盖了原始循环的前768次迭代。通过启动包含更多块的网格，可以处理更大的向量。通过启动包含n或更多线程的网格，可以处理长度为n的向量。

图2.10显示了一个用于向量加法的核心函数。注意，我们在核心函数中不使用“_h”和“_d”的约定，因为不会产生混淆。在我们的示例中，我们将不会访问主机内存。核心函数的语法是ANSI C，带有一些值得注意的扩展。首先，在vecAddKernel函数的声明前有一个CUDA-C特定的关键字“__global__”。这个关键字表明该函数是一个核心函数，并且可以被调用以在设备上生成线程网格。

CUDA C通过三个限定符关键字扩展了C语言，这些关键字可以用于函数声明。这些关键字的含义总结在图2.11中。"__global__"关键字表明正在声明的函数是一个CUDA C核心函数。注意"global"这个词两边各有两个下划线字符。这样的核心函数在设备上执行，并且可以从主机调用。在支持动态并行的CUDA系统中，它也可以从设备调用，正如我们将在第21章"CUDA动态并行"中看到的那样。重要的特点是调用这样的核心函数会导致在设备上启动一个新的线程网格。

![image-20240718151605564](imgs/image-20240718151605564.png)

"__device__"关键字表明正在声明的函数是一个CUDA设备函数。设备函数在CUDA设备上执行，并且只能从核心函数或其他设备函数中调用。设备函数由调用它的设备线程执行，并且不会导致启动任何新的设备线程。

"__host__"关键字表明正在声明的函数是一个CUDA主机函数。主机函数是一个传统的C函数，它在主机上执行，并且只能从另一个主机函数中调用。默认情况下，如果它们的声明中没有任何CUDA关键字，CUDA程序中的所有函数都是主机函数。这是有意义的，因为许多CUDA应用程序是从仅CPU执行环境移植过来的。程序员在移植过程中会添加核心函数和设备函数。原始函数仍然作为主机函数保留。让所有函数默认为主机函数可以节省程序员修改所有原始函数声明的繁琐工作。

需要注意的是，可以在函数声明中同时使用“__host__”和“__device__”。这种组合告诉编译系统为同一个函数生成两个版本的目标代码。一个在主机上执行，并且只能从主机函数中调用。另一个在设备上执行，并且只能从设备或核心函数中调用。这支持了一个常见的用例，即相同的函数源代码可以重新编译以生成设备版本。许多用户库函数可能会属于这一类。

图2.10中的第二个值得注意的C语言扩展是内置变量“threadIdx”、“blockIdx”和“blockDim”。回想一下，所有线程都执行相同的核心代码，需要有一种方式让它们相互区分，并将每个线程引导到数据的特定部分。这些内置变量是线程访问提供标识坐标的硬件寄存器的方式。不同的线程在其threadIdx.x、blockIdx.x和blockDim.x变量中会看到不同的值。为了可读性，我们有时会在讨论中将一个线程称为$thread_{blockIdx.x、threadIdx.x}$。

在图2.10中，有一个自动（局部）变量i。在CUDA核心函数中，自动变量是每个线程私有的。也就是说，将为每个线程生成i的一个版本。如果网格以10,000个线程启动，将会有10,000个i的版本，每个线程一个。一个线程分配给它的i变量的值对其他线程是不可见的。我们将在第5章“内存架构和数据局部性”中更详细地讨论这些自动变量。

通过比较图2.4和图2.10，可以揭示CUDA核心函数的一个重要见解。图2.10中的核心函数没有与图2.4中的循环相对应的循环。读者应该问循环去哪儿了。答案是循环现在被线程网格取代了。整个网格形成了循环的等价物。网格中的每个线程对应原始循环的一个迭代。这有时被称为循环并行性，其中原始顺序代码的迭代由线程并行执行。

注意，在图2.10中的addVecKernel有一个if (i < n)语句。这是因为并非所有向量长度都可以表示为块大小的倍数。例如，假设向量长度为100。最小的有效线程块维度是32。假设我们选择32作为块大小。需要启动四个线程块来处理所有100个向量元素。然而，这四个线程块将有128个线程。我们需要禁用第三个线程块中的最后28个线程执行原始程序不期望的工作。由于所有线程都要执行相同的代码，所有线程都将测试它们的i值与n（即100）的关系。有了if (i < n)语句，前100个线程将执行加法，而最后28个则不会。这允许核心函数被调用以处理任意长度的向量。

## 2.6调用内核函数

![image-20240718152133998](imgs/image-20240718152133998.png)

实现了核心函数后，剩余的步骤是从主机代码中调用该函数以启动网格。这在图2.12中有所说明。当主机代码调用核心函数时，它通过执行配置参数设置网格和线程块的维度。配置参数在传统C函数参数之前的“<<< , , >>>”和“...”之间给出。第一个配置参数给出了网格中的块数。第二个指定了每个块中的线程数。在这个例子中，每个块中有256个线程。为了确保我们有足够的线程覆盖所有的向量元素，我们需要将网格中的块数设置为所需线程数（在这种情况下是n）除以线程块大小（在这种情况下是256）的向上取整（将商向上取整到最近的更高整数值）。执行向上取整除法有很多方法。一种方法是将C语言的向上取整函数应用于n/256.0。使用浮点值256.0确保我们为除法生成一个浮点值，以便向上取整函数可以正确地向上取整。例如，如果我们想要1000个线程，我们将启动ceil(1000/256.0) = 4个线程块。因此，该语句将启动4 * 256 = 1024个线程。在核心函数中如图2.10所示的if (i < n)语句，前1000个线程将对1000个向量元素执行加法。其余的24个线程将不会执行。

![image-20240718152219809](imgs/image-20240718152219809.png)

图 2.13 显示了 vecAdd 函数中的最终主机代码。这段源代码完成了图 2.5 中的框架。图 2.12 和 2.13 共同展示了一个简单的 CUDA 程序，它由主机代码和一个设备内核组成。代码被硬编码为使用每个线程块 256 个线程。 然而，使用的线程块数量取决于向量的长度（n）。如果 n 是 750，将使用三个线程块。如果 n 是 4000，将使用 16 个线程块。如果 n 是 2,000,000，则将使用 7813 个线程块。需要注意的是，所有线程块都在向量的不同部分上操作。它们可以以任意顺序执行。程序员不得对执行顺序做出任何假设。一个拥有少量执行资源的小 GPU 可能只能并行执行一个或两个这些线程块。一个更大的 GPU 可能并行执行 64 或 128 个线程块。这使得 CUDA 内核在执行速度上具有可扩展性。也就是说，相同的代码在小 GPU 上运行速度较慢，在大 GPU 上运行速度较快。我们将在第 4 章“计算架构和调度”中再次讨论这一点。

再次强调，向量加法示例之所以被使用，是因为它简单。在实践中，分配设备内存、从主机到设备输入数据传输、从设备到主机输出数据传输以及释放设备内存的开销可能会使最终代码比图 2.4 中的原始顺序代码慢。这是因为内核执行的计算量相对于处理或传输的数据量来说是很小的。对于两个浮点输入操作数和一个浮点输出操作数，只执行一次加法。实际应用通常具有内核，相对于处理的数据量，需要更多的工作，这使得额外的开销是值得的。实际应用也倾向于在多次内核调用之间保持数据在设备内存中，以便可以分摊开销。我们将展示几个这样的应用示例。

## 2.7 编译

我们已经看到，实现 CUDA C 内核需要使用各种扩展，这些扩展不是 C 语言的一部分。一旦这些扩展在代码中使用，传统的 C 编译器就不再接受它。代码需要由一个能够识别和理解这些扩展的编译器来编译，比如 NVCC（NVIDIA C 编译器）。正如图 2.14 顶部所示，NVCC 编译器处理一个 CUDA C 程序，使用 CUDA 关键字来分隔主机代码和设备代码。主机代码是直接的 ANSI C 代码，使用主机的标准 C/C++ 编译器进行编译，并作为传统的 CPU 进程运行。设备代码，用 CUDA 关键字标记，这些关键字指定 CUDA 内核及其相关的辅助函数和数据结构，由 NVCC 编译成称为 PTX 文件的虚拟二进制文件。这些 PTX 文件进一步由 NVCC 的运行时组件编译成真实的对象文件，并在支持 CUDA 的 GPU 设备上执行。

![image-20240718152700791](imgs/image-20240718152700791.png)

## 2.8总结

### 2.8.1 函数声明
CUDA C 扩展了 C 函数声明语法以支持异构并行计算。这些扩展在图 2.12 中进行了总结。使用 " __global__ "、" __device__ " 或 " __host__ " 中的一个，CUDA C 程序员可以指示编译器生成内核函数、设备函数或主机函数。所有没有这些关键字的函数声明默认为主机函数。如果一个函数声明中同时使用了 " __host__ " 和 " __device__ "，则编译器会为设备和主机生成两个版本的函数。如果一个函数声明没有任何 CUDA C 扩展关键字，则函数默认为主机函数。

### 2.8.2 内核调用和网格启动
CUDA C 扩展了 C 函数调用语法，通过在<< >>中包含内核执行配置参数。这些执行配置参数仅在调用内核函数以启动网格时使用。我们讨论了定义网格维度和每个块维度的执行配置参数。读者应参考 CUDA 编程指南（NVIDIA，2021）以获取有关内核启动扩展以及其他类型的执行配置参数的更多详细信息。

### 2.8.3 内置（预定义）变量
CUDA 内核可以访问一组内置的、预定义的只读变量，这些变量允许每个线程区分自己与其他线程，并确定要处理的数据区域。我们在本章讨论了 threadIdx、blockDim 和 blockIdx 变量。在第 3 章“多维网格和数据”中，我们将讨论使用这些变量的更多细节。

### 2.8.4运行时应用程序编程接口

CUDA 提供了一系列 API 函数，为 CUDA C 程序提供服务。本章中我们讨论了 cudaMalloc、cudaFree 和 cudaMemcpy 函数。这些函数由主机代码调用，分别用于分配设备全局内存、释放设备全局内存以及代表调用程序在主机和设备之间传输数据。读者可以参考 CUDA C 编程指南了解更多其他 CUDA API 函数。

本章的目标是介绍 CUDA C 的核心概念以及编写简单 CUDA C 程序所必需的 CUDA 扩展。本章绝不是 CUDA 所有特性的全面概述。书中的其余部分将涵盖其中的一些特性。然而，我们将重点放在这些特性所支持的关键并行计算概念上。我们将只介绍并行编程技术示例代码中所需的 CUDA C 特性。通常，我们希望鼓励读者始终参考 CUDA C 编程指南以获取更多 CUDA C 特性的详细信息。

# 3 多维网格和数据

在第2章“异构数据并行计算”中，我们学习了如何编写一个简单的CUDA C 11程序，通过调用内核函数来启动一维线程网格，以操作一维数组的元素。内核指定了网格中每个单独线程执行的语句。在本章中，我们将更一般地了解线程是如何组织的，并学习如何使用线程和块来处理多维数组。整个章节中将使用多个示例，包括将彩色图像转换为灰度图像、模糊图像和矩阵乘法。这些示例还有助于让读者在讨论即将到来的GPU架构、内存组织和性能优化之前，熟悉数据并行性推理。

## 3.1多维网格组织

在 CUDA 中，网格中的所有线程都执行相同的内核函数，并且它们依赖于坐标，即线程索引，来区分彼此并识别要处理的数据的适当部分。正如我们在第 2 章“异构数据并行计算”中看到的，这些线程被组织成两级层次结构：一个网格由一个或多个块组成，每个块由一个或多个线程组成。一个块中的所有线程共享相同的块索引，可以通过 blockIdx（内置）变量访问。每个线程还有一个线程索引，可以通过 threadIdx（内置）变量访问。当一个线程执行内核函数时，对 blockIdx 和 threadIdx 变量的引用将返回线程的坐标。内核调用语句中的执行配置参数指定了网格的维度和每个块的维度。这些维度可以通过 gridDim 和 blockDim（内置）变量获得。

在一般情况下，网格是一个三维（3D）的块数组，每个块是一个三维（3D）的线程数组。在调用内核时，程序需要指定网格和每个维度的块的大小。这些通过内核调用语句的执行配置参数（在<< >>中...）来指定。第一个执行配置参数指定了网格的维度，以块的数量来表示。第二个参数指定了每个块的维度，以线程的数量来表示。每个这样的参数都是dim3类型，这是一个包含三个元素x、y和z的整数向量类型。这三个元素指定了三个维度的大小。程序员可以通过将未使用的维度的大小设置为1来使用少于三个维度。

例如，以下主机代码可以用来调用 `vecAddkernel()` 内核函数，并生成一个1D网格，该网格由32个块组成，每个块包含128个线程。网格中的总线程数是 $(128 \times 32 = 4096)$。

![image-20240718163003488](imgs/image-20240718163003488.png)

请注意，dimBlock 和 dimGrid 是由程序员定义的主机代码变量。这些变量可以有任何合法的 C 变量名称，只要它们具有 dim3 类型。例如，以下语句与上述语句达到相同的结果：

![image-20240718163040240](imgs/image-20240718163040240.png)

网格和块的维度也可以从其他变量计算得出。例如，图 2.12 中的内核调用可以写成以下形式

![image-20240718163149662](imgs/image-20240718163149662.png)

这允许块的数量根据向量的大小变化，以便网格有足够的线程来覆盖所有向量元素。在这个例子中，程序员选择将块大小固定在256。内核调用时变量 n 的值将决定网格的维度。如果 n 等于1000，网格将由四个块组成。如果 n 等于4000，网格将有16个块。在每种情况下，都有足够的线程来覆盖所有的向量元素。一旦网格被启动，网格和块的维度将保持不变，直到整个网格完成执行。

为了方便，CUDA 提供了一个特殊捷径来调用具有一维（1D）网格和块的内核。而不是使用 dim3 变量，可以使用算术表达式来指定1D网格和块的配置。在这种情况下，CUDA 编译器简单地将算术表达式作为 x 维度，并假定 y 和 z 维度为 1。这给我们带来了图 2.12 所示的内核调用语句：

![image-20240718163308758](imgs/image-20240718163308758.png)

了解 C++ 的读者会意识到，这种“简写”约定利用了 C++ 构造函数和默认参数的工作原理。`dim3` 构造函数的参数默认值是 1。当在期望 `dim3` 的地方传递单个值时，该值将传递给构造函数的第一个参数，而第二和第三个参数则取默认值 1。结果是创建了一个一维网格或块，其中 x 维度的大小是传递的值，而 y 和 z 维度的大小为 1。

在内核函数内部，变量 `gridDim` 和 `blockDim` 的 x 字段根据执行配置参数的值预先初始化。例如，如果 n 等于 4000，那么在 `vectAddkernel` 内核中对 `gridDim.x` 和 `blockDim.x` 的引用将分别产生 16 和 256。需要注意的是，与主机代码中的 `dim3` 变量不同，这些变量在内核函数内的名称是 CUDA C 规范的一部分，不能更改。也就是说，`gridDim` 和 `blockDim` 是内核中的内置变量，始终反映网格和块的维度。

在 CUDA C 中，gridDim.x 的允许值范围从 1 到 \(2^{31} - 2\)，而 gridDim.y 和 gridDim.z 的允许值范围从 1 到 \(2^{16} - 1\)（即 65,535）。一个块内的所有线程共享相同的 blockIdx.x、blockIdx.y 和 blockIdx.z 值。在不同的块之间，blockIdx.x 的值范围从 0 到 gridDim.x-1，blockIdx.y 的值范围从 0 到 gridDim.y-1，blockIdx.z 的值范围从 0 到 gridDim.z-1。

我们现在转向块的配置。每个块被组织成一个三维的线程数组。可以通过将 blockDim.z 设置为 1 来创建二维块。通过将 blockDim.y 和 blockDim.z 都设置为 1，可以创建一维块，如 vectorAddkernel 示例所示。正如我们之前提到的，网格中的所有块具有相同的维度和大小。块中每个维度的线程数由内核调用时的第二个执行配置参数指定。在内核内，这个配置参数可以作为 blockDim 的 x、y 和 z 字段访问。

当前 CUDA 系统中，一个块的总大小限制为 1024 个线程。这些线程可以以任何方式分布在三个维度上，只要总线程数不超过 1024。例如，blockDim 值 (512, 1, 1)、(8, 16, 4) 和 (32, 16, 2) 都是允许的，但是 (32, 32, 2) 是不允许的，因为总线程数将超过 1024。

![image-20240718163734667](imgs/image-20240718163734667.png)

网格和它的块不需要具有相同的维度。网格可以具有比其块更高的维度，反之亦然。例如，图 3.1 显示了一个具有 gridDim (2, 2, 1) 和 blockDim (4, 2, 2) 的小型玩具网格示例。这样的网格可以使用以下主机代码创建：

![image-20240718163710640](imgs/image-20240718163710640.png)

图 3.1 中的网格由四个块组成，这些块被组织成一个 2x2 的数组。每个块用 (blockIdx.y, blockIdx.x) 进行标记。例如，块 (1,0) 的 blockIdx.y 为 1，blockIdx.x 为 0。注意，块和线程标签的顺序是最高维度在前。这种标记方法与 C 语句中设置配置参数时使用的顺序相反，后者是最低维度在前。这种相反的标记顺序在说明线程坐标映射到多维数据索引时更有效。

每个 threadIdx 也包含三个字段：x 坐标 threadId.x，y 坐标 threadIdx.y，以及 z 坐标 threadIdx.z。图 3.1 展示了块内线程的组织方式。在这个例子中，每个块被组织成 4x2x2 的线程数组。由于网格内所有块的维度相同，我们只展示了其中一个。图 3.1 展开块 (1,1) 来展示它的 16 个线程。例如，线程 (1,0,2) 的 threadIdx.z 为 1，threadIdx.y 为 0，threadIdx.x 为 2。注意，在这个例子中我们有 4 个块，每个块有 16 个线程，总共有 64 个线程在网格中。我们使用这些小数字以保持插图简单。典型的 CUDA 网格包含数千到数百万线程。

## 3.2 将线程映射到多维数据

选择一维（1D）、二维（2D）或三维（3D）线程组织通常基于数据的性质。例如，图片是像素的二维数组。使用由二维块组成的二维网格通常方便于处理图片中的像素。图 3.2 显示了处理一个 62x76 像素的图片 P 的这种安排（垂直或 y 方向上有 62 个像素，水平或 x 方向上有 76 个像素）。假设我们决定使用 16x16 的块，x 方向上有 16 个线程，y 方向上有 16 个线程。我们需要 y 方向上的四个块和 x 方向上的五个块，这将产生 4x5，共 20 个块，如图 3.2 所示。粗线标记了块的边界。阴影区域描绘了覆盖像素的线程。每个线程被分配处理一个像素，其 y 和 x 坐标是从其 blockIdx、blockDim 和 threadIdx 变量值派生出来的：

![image-20240718164355134](imgs/image-20240718164355134.png)

[^]: 我们将按降序引用多维数据的维数：z维数后跟y维数，以此类推。例如，对于垂直或y维度的n个像素和水平或x维度的m个像素的图片，我们将其称为n 3 m的图片。这遵循了C多维数组索引约定。例如，为了简洁起见，我们可以在文本和数字中将x称为Py，x。不幸的是，这种排序顺序与数据维度在网格维度和块维度中的排序顺序相反。当我们根据要由线程处理的多维数组来定义线程网格的维度时，这种差异可能尤其令人困惑。

![image-20240718163829605](imgs/image-20240718163829605.png)

例如，由块 (1,0) 中的线程 (0,0) 要处理的像素元素可以按以下方式确定：

![image-20240718164745820](imgs/image-20240718164745820.png)

请注意，在图 3.2 中，我们在 y 方向上有两个额外的线程，在 x 方向上有四个额外的线程。也就是说，我们将生成 64x80 个线程来处理 62x76 个像素。这类似于在图 2.9 中使用四个 256 线程块的一维内核 vecAddKernel 处理 1000 元素向量的情况。回想一下，在图 2.10 中需要一个 if 语句来防止额外的 24 个线程产生影响。类似地，我们应该预期图像处理内核函数将有 if 语句来测试线程的垂直和水平索引是否落在像素的有效范围内。

我们假设主机代码使用一个整数变量 n 来跟踪 y 方向上的像素数，另一个整数变量 m 来跟踪 x 方向上的像素数。我们进一步假设输入图像数据已经被复制到设备全局内存，并且可以通过指针变量 Pin_d 访问。输出图像已经在设备内存中分配，并且可以通过指针变量 Pout_d 访问。以下主机代码可以用来调用一个 2D 内核 colorToGrayscaleConversion 来处理图像，如下所示：

![image-20240718164846043](imgs/image-20240718164846043.png)

在本例中，我们为了简单起见假设块的尺寸固定为 16x16。另一方面，网格的尺寸取决于图片的尺寸。为了处理一个 1500x2000（300 万像素）的图片，我们将生成 11,750 个块：y 方向上有 94 个块，x 方向上有 125 个块。在内核函数中，对 gridDim.x、gridDim.y、blockDim.x 和 blockDim.y 的引用将分别产生 125、94、16 和 16。

在我们展示内核代码之前，我们首先需要理解 C 语句如何访问动态分配的多维数组的元素。理想情况下，我们希望能够将 Pin_d 作为一个二维数组访问，其中行 j 和列 i 的元素可以作为 Pin_d[j][i] 访问。然而，基于 ANSI C 标准的 CUDA C 开发要求在编译时就知道 Pin 中的列数，以便将 Pin 作为二维数组访问。不幸的是，对于动态分配的数组来说，编译时并不知道这个信息。实际上，使用动态分配数组的部分原因正是为了允许这些数组的大小和维度根据运行时的数据大小而变化。

因此，设计上编译时并不知道动态分配的二维数组的列数。结果，程序员需要明确地将动态分配的二维数组“展平”或“线性化”为等价的一维数组，这在当前的 CUDA C 中是必需的。

实际上，C 中的所有多维数组都是线性化的。这是由于现代计算机使用“平面”内存空间（见“内存空间”侧边栏）。在静态分配的数组的情况下，编译器允许程序员使用更高维度的索引语法，例如 Pin_d[j][i]，来访问它们的元素。在幕后，编译器将它们线性化为等价的一维数组，并将多维索引语法转换为一维偏移。在动态分配的数组的情况下，由于编译时缺乏维度信息，当前的 CUDA C 编译器将这种转换工作留给了程序员。

> 内存空间
> 内存空间是现代计算机中处理器访问其内存的简化视图。通常，每个运行的应用程序都与一个内存空间相关联。应用程序要处理的数据和执行的指令都存储在其内存空间的位置中。每个位置通常可以容纳一个字节，并具有一个地址。需要多个字节的变量——浮点数4个字节，双精度数8个字节——存储在连续的字节位置中。当从内存空间访问数据值时，处理器给出起始地址（起始字节位置的地址）和所需的字节数。
> 大多数现代计算机至少有4G字节大小的位置，其中每个G是1,073,741,824（2的30次方）。所有位置都标有从0到最大使用数字的地址。由于每个位置只有一个地址，我们说内存空间具有“平坦”的组织。因此，所有多维数组最终都被“展平”成等价的一维数组。虽然C程序员可以使用多维数组语法来访问多维数组的元素，但编译器将这些访问转换为指向数组起始元素的基本指针，以及从这些多维索引计算出的一维偏移量。

有两种方法可以将二维数组线性化。一种是将同一行的所有元素放置在连续的位置。然后将这些行一个接一个地放入内存空间。这种排列方式称为行主序布局，如图3.3所示。为了提高可读性，我们使用Mj,i来表示M的第j行第i列的元素。Mj,i等同于C语言表达式M[j][i]，但更易于阅读。图3.3展示了一个4x3x4矩阵M被线性化为一个16元素的一维数组的例子，首先是第0行的所有元素，然后是第1行的四个元素，以此类推。

因此，M的第j行第i列的元素的一维等价索引是$j \times 4 + i$。j乘以4的项跳过第j行之前的所有行的元素。然后i项在选择第j行部分内的正确元素。例如，M2,1的一维索引是$2 \times 4 + 1 = 9$。这在图3.3中被说明，其中M9是M2,1的一维等价。这是C编译器线性化二维数组的方式。

![image-20240718165658884](imgs/image-20240718165658884.png)

另一种将二维数组线性化的方法是将同一列的所有元素放置在连续的位置。然后这些列一个接一个地放入内存空间。这种排列方式称为列主序布局，由FORTRAN编译器使用。请注意，二维数组的列主序布局等同于其转置形式的行主序布局。我们不会在这个问题上花费更多时间，只是提醒那些主要以前的编程经验是使用FORTRAN的读者，CUDA C使用行主序布局而不是列主序布局。此外，许多设计用于FORTRAN程序的C库使用列主序布局以匹配FORTRAN编译器的布局。因此，这些库的手册页面通常告诉用户，如果他们从C程序调用这些库，就需要转置输入数组。

![image-20240718165755452](imgs/image-20240718165755452.png)

我们现在准备研究图3.4中显示的colorToGrayscaleConversion的源代码。内核代码使用以下公式将每个颜色像素转换为其灰度对应物：

![image-20240718170347429](imgs/image-20240718170347429.png)

在水平方向上总共有blockDim.x * gridDim.x个线程。 类似于vecAddKernel示例，以下表达式生成从0到blockDim.x * gridDim.x - 1的所有整数值（第06行）：![image-20240718170405008](imgs/image-20240718170405008.png)

我们知道gridDim.x * blockDim.x大于或等于宽度（从主机代码传入的m值）。我们在水平方向上的线程数量至少与像素数量一样多。我们还知道在垂直方向上的线程数量至少与像素数量一样多。因此，只要我们测试并确保只有行和列值都在范围内的线程，即(col < width) && (row < height)，我们就能够覆盖图片中的每个像素（第07行）。

由于每行有width个像素，我们可以生成行号为row、列号为col的像素的一维索引，公式为 row * width + col（第10行）。这个一维索引grayOffset是输出灰度图像Pout的像素索引，因为输出灰度图像中的每个像素是1个字节（无符号字符）。以我们的62x76图像为例，使用以下公式计算块(1,0)中线程(0,0)计算的Pout像素的线性化一维索引：

![image-20240718170637484](imgs/image-20240718170637484.png)

对于输入图像Pin，我们需要将灰度像素索引乘以3（第13行），因为每个彩色像素存储为三个元素（r, g, b），每个元素都是1个字节。得到的rgbOffset给出了Pin数组中彩色像素的起始位置。我们从Pin数组的三个连续字节位置读取r、g和b的值（第14-16行），执行灰度像素值的计算，并将该值写入Pout数组，使用grayOffset（第19行）。以我们的62x76图像为例，由块(1,0)中的线程(0,0)处理的Pin像素的第一个分量的线性化一维索引可以通过以下公式计算

![image-20240718171058542](imgs/image-20240718171058542.png)

正在访问的数据是从字节偏移量3648开始的3个字节。

![image-20240718171134346](imgs/image-20240718171134346.png)

图3.5展示了在处理我们的62x76示例时colorToGrayscaleConversion的执行情况。假设有16x16的块，调用colorToGrayscaleConversion内核将生成64x80个线程。网格将有4x5x5=20个块：垂直方向有四个，水平方向有五个。块的执行行为将分为四种不同的情况，如图3.5中的四个阴影区域所示。

图3.5中标记为1的第一个区域包括属于覆盖图片中大多数像素的12个块的线程。这些线程的col和row值都在范围内；所有这些线程都通过了if语句测试，并在图片的暗阴影区域处理像素。也就是说，每个块中的16x16=256个线程将处理像素。

图3.5中标记为2的第二个区域包含属于中等阴影区域的三个块的线程，这些区域覆盖了图片的右上角像素。尽管这些线程的row值始终在范围内，但其中一些线程的col值超过了76的m值。这是因为水平方向上的线程数始终是程序员选择的blockDim.x值（在这种情况下是16）的倍数。覆盖76个像素所需的16的最小倍数是80。因此，每行的12个线程将发现它们的col值在范围内，并将处理像素。每行剩余的四个线程将发现它们的col值超出范围，因此将不满足if语句条件。这些线程将不处理任何像素。总的来说，每个块中的16x16=256个线程中有12x16=192个线程将处理像素。

图3.5中标记为3的第三个区域涵盖了覆盖图片中等阴影区域的四个左下角块。尽管这些线程的col值始终在范围内，但其中一些线程的row值超过了62的n值。这是因为垂直方向上的线程数始终是程序员选择的blockDim.y值（在这种情况下是16）的倍数。覆盖62的16的最小倍数是64。因此，每列的14个线程将发现它们的row值在范围内，并将处理像素。每列剩余的两个线程将不会通过if语句，也不会处理任何像素。总的来说，256个线程中有16x14=224个线程将处理像素。

图3.5中标记为4的第四个区域包含了覆盖图片右下角，浅色阴影区域的线程。像区域2一样，顶部14行中的每个4个线程将发现它们的col值超出范围。像区域3一样，这个块的整个底部两行将发现它们的row值超出范围。总的来说，只有16x16=256中的14x12=168个线程将处理像素。

我们可以很容易地将我们对二维数组的讨论扩展到三维数组，通过在数组线性化时增加另一个维度。这是通过将数组的每个“平面”一个接一个地放入地址空间来完成的。假设程序员使用变量m和n分别跟踪三维数组中的列数和行数。程序员还需要在调用内核时确定blockDim.z和gridDim.z的值。在内核中，数组索引将涉及另一个全局索引：

![image-20240718172221587](imgs/image-20240718172221587.png)

三维数组P的线性化访问形式为 P[plane * m * n + row * m + col]。处理三维P数组的内核需要检查三个全局索引，即plane、row和col，是否落在数组的有效范围内。在CUDA内核中使用三维数组将在第8章“模板”中进一步研究，特别是对于模板模式。

## 3.3图像模糊：一个更复杂的内核

![image-20240718175529396](imgs/image-20240718175529396.png)

我们已经研究了vecAddkernel和colorToGrayscaleConversion，其中每个线程只在数组元素上执行少量的算术运算。这些内核很好地完成了它们的目的：说明基本的CUDA C程序结构和数据并行执行概念。在这一点上，读者应该提出一个明显的问题：CUDA C程序中的所有线程是否只执行这样简单和琐碎的操作，并且彼此独立？答案是不。在真正的CUDA C程序中，线程经常在它们的数据上执行复杂操作，并且需要相互合作。在接下来的几章中，我们将要处理越来越复杂的例子，这些例子表现出这些特征。我们将从图像模糊函数开始。

图像模糊平滑了像素值的突然变化，同时保留了对识别图像关键特征至关重要的边缘。图3.6展示了图像模糊的效果。简单地说，我们使图像变得模糊。对人类眼睛来说，模糊的图像倾向于掩盖细节，呈现出“大局”印象，或者是图像中的主要主题对象。在计算机图像处理算法中，图像模糊的常见用例是通过用干净的周围像素值校正有问题的像素值，来减少图像中噪声和颗粒渲染效果的影响。在计算机视觉中，图像模糊可以用来让边缘检测和对象识别算法专注于主题对象，而不是被大量细粒度的对象所困扰。在显示中，有时使用图像模糊来突出图像的某个部分，通过使图像的其余部分变得模糊。

数学上，图像模糊函数计算输出图像像素的值为输入图像中包围该像素的像素块的加权和。正如我们将在第7章“卷积”中学到的，这种加权和的计算属于卷积模式。在本章中，我们将采用简化的方法，取目标像素周围（包括目标像素本身）的NxN像素块的简单平均值。为了保持算法的简单性，我们不会根据任何像素与目标像素的距离来赋予其权重。在实践中，卷积模糊方法（如高斯模糊）中放置这样的权重是非常常见的。

![image-20240718182134921](imgs/image-20240718182134921.png)

图3.7展示了使用3x3像素块进行图像模糊的一个例子。在计算(row, col)位置的输出像素值时，我们可以看到该像素块以位于(row, col)位置的输入像素为中心。3x3像素块覆盖了三行（row-1，row，row+1）和三列（col-1，col，col+1）。例如，计算(25, 50)位置的输出像素的九个像素的坐标分别是(24, 49)，(24, 50)，(24, 51)，(25, 49)，(25, 50)，(25, 51)，(26, 49)，(26, 50)和(26, 51)。

![image-20240718182230027](imgs/image-20240718182230027.png)

图3.8展示了一个图像模糊内核。类似于在colorToGrayscaleConversion中使用的方法，我们使用每个线程来计算一个输出像素。也就是说，线程到输出数据的映射保持不变。因此，在内核开始时我们看到熟悉的计算col和row索引的代码（第03-04行）。我们还看到了熟悉的if语句，它验证col和row都在图像的高度和宽度的有效范围内（第05行）。只有col和row索引都在值范围内的线程才被允许参与执行。

图3.7显示了用于计算线程输出像素的输入像素块的中心像素位置的col和row值。图3.8中的嵌套for循环（第10-11行）遍历块中的所有像素。我们假设程序定义了一个常量BLUR_SIZE。BLUR_SIZE的值设置为块每侧（半径）的像素数，2 * BLUR_SIZE + 1给出块的一个维度上的总像素数。例如，对于一个3x3的块，BLUR_SIZE设置为1，而对于一个7x7的块，BLUR_SIZE设置为3。外层循环遍历块的行。对于每一行，内层循环遍历块的列。

在我们3x3的块示例中，BLUR_SIZE是1。对于计算输出像素(25, 50)的线程，在外部循环的第一次迭代中，curRow变量是row - BLUR_SIZE = (25 - 2 * 1) = 23。因此，在外部循环的第一次迭代中，内部循环遍历第23行的块像素。内部循环使用curCol变量从列col - BLUR_SIZE = 50 - 1 = 49迭代到col + BLUR_SIZE = 51。

因此，在外部循环的第一次迭代中处理的像素是(24, 49)，(24, 50)和(24, 51)。读者应该验证在外部循环的第二次迭代中，内部循环遍历像素(25, 49)，(25, 50)和(25, 51)。最后，在外部循环的第三次迭代中，内部循环遍历像素(26, 49)，(26, 50)和(26, 51)。

第16行使用curRow和curCol的线性索引来访问当前迭代中访问的输入像素的值。它将像素值累积到一个运行中的总和变量pixVal中。第17行通过增加pixels变量来记录一个像素值已经被添加到运行中的总和中。在处理完块中的所有像素后，第22行通过将pixVal值除以pixels值来计算块中像素的平均值。它使用row和col的线性索引将结果写入其输出像素。

![image-20240718182444389](imgs/image-20240718182444389.png)

图3.9展示了使用3x3像素块进行图像模糊时，边缘像素的处理情况。第15行包含了一个条件语句，用于保护第16行和第17行的执行。例如，在计算图像边缘附近的输出像素时，像素块可能会超出输入图像的有效范围。图3.9假设使用3x3像素块进行了说明。在情况1中，正在模糊左上角的像素。预期像素块中的九个像素中有五个在输入图像中不存在。在这种情况下，输出像素的row和col值分别为0和0。在嵌套循环的执行过程中，九次迭代的curRow和curCol值分别为(21, -1)，(21,0)，(21,1)，(-1, -1)，(-1,0)，(-1,1)，(0, -1)，(0,0)和(0,1)。请注意，对于图像外的五个像素，至少有一个值小于0。if语句中的curRow < 0和curCol < 0条件捕获这些值，并跳过第16行和第17行的执行。因此，只有四个有效像素的值被累积到运行中的总和变量中。pixels值也正确地只增加了四次，以便在第22行正确计算平均值。

读者应该通过图3.9中的其他情况，并分析blurKernel中嵌套循环的执行行为。请注意，大多数线程会发现它们分配的3x3像素块中的所有像素都在输入图像内。它们将累积所有的九个像素。然而，对于四个角落上的像素，负责的线程将只累积四个像素。对于四个边缘上的其他像素，负责的线程将累积六个像素。这些变化是需要使用变量pixels跟踪实际累积的像素数量的原因。

## 3.4 矩阵乘法

矩阵-矩阵乘法，简称矩阵乘法，是基础线性代数子程序标准（参见“线性代数函数”侧边栏）的重要组成部分。它是许多线性代数求解器的基础，例如LU分解。它也是深度学习中使用卷积神经网络的重要计算，将在第16章“深度学习”中详细讨论。

> 线性代数函数
> 线性代数运算在科学和工程应用中被广泛使用。在基础线性代数子程序（BLAS）中，这是一个发布执行基本代数运算库的事实上的标准，有三种级别的线性代数函数。随着级别的增加，函数执行的操作数量也在增加。一级函数执行形式为 y = αx + y 的向量运算，其中 x 和 y 是向量，α 是标量。我们的向量加法示例是一级函数的一个特殊情况，其中 α = 1。二级函数执行形式为 y = αAx + βy 的矩阵-向量运算，其中 A 是矩阵，x 和 y 是向量，α 和 β 是标量。我们将在稀疏线性代数中研究二级函数的一种形式。三级函数执行形式为 C = αAB + βC 的矩阵-矩阵运算，其中 A、B 和 C 是矩阵，α 和 β 是标量。我们的矩阵-矩阵乘法示例是三级函数的一个特殊情况，其中 α = 1 且 β = 0。这些 BLAS 函数之所以重要，是因为它们被用作更高级代数函数的基本构建块，如线性系统求解器和特征值分析。正如我们将在后面讨论的，不同实现的 BLAS 函数的性能在顺序和并行计算机中可能会有数量级的差异。

矩阵乘法是将一个 𝐼×𝐽（即有 I 行 𝐽 列）的矩阵 𝑀*M* 与一个 J*×*K 的矩阵 𝑁 相乘，得到一个 I*×*K 的矩阵 P*。在执行矩阵乘法时，输出矩阵 P中的每个元素都是 M 的一行与 N的一列的内积。我们将继续使用这样的约定：Prow,col 是在垂直方向上第 row行和水平方向上第 col 列的元素。如图 3.10 所示，𝑃row,col（在 P中的小方块）是 M 中第 row 行形成的向量（在 M* 中显示为水平条带）与 N中第 col 列形成的向量（在 N 中显示为垂直条带）的内积。两个向量的内积，有时也称为点积，是各个向量元素乘积的总和。即![image-20240719151103801](imgs/image-20240719151103801.png)

![image-20240719151122776](imgs/image-20240719151122776.png)

例如，在图 3.10 中，假设行号为 1 且列号为 5：

![image-20240719151211118](imgs/image-20240719151211118.png)

使用CUDA实现矩阵乘法时，我们可以将网格中的线程映射到输出矩阵P的元素上，采用与colorToGrayscaleConversion相同的方法。也就是说，每个线程负责计算一个P元素。每个线程要计算的P元素的行和列索引与之前相同：

![image-20240719151301338](imgs/image-20240719151301338.png)

![image-20240719151313286](imgs/image-20240719151313286.png)

基于这种一对一映射，行和列线程索引也是它们输出元素的行和列索引。图3.11显示了基于这种线程到数据映射的内核源代码。读者应该立即看到计算行和列的熟悉模式（第03和04行），以及测试行和列是否都在范围内的if语句（第05行）。这些语句几乎与colorToGrayscaleConversion中的对应语句相同。唯一的显著区别在于，我们做了一个简化的假设，即matrixMulKernel只需要处理正方形矩阵，因此我们用Width替换了宽度和高度。这种线程到数据的映射有效地将P划分为瓦片，其中一个在图3.10中显示为浅色方格。每个区块负责计算这些瓦片中的一个。

现在我们来关注每个线程所做的工作。回想一下，$P_{\text{row},\text{col}} $ 是通过计算矩阵 \( M \) 的第 $\text{row} $ 行和矩阵 \( N \) 的第 $ \text{col} $ 列的内积来计算的。在图3.11中，我们使用一个for循环来执行这个内积操作。在我们进入循环之前，我们将一个局部变量 $ P_{\text{value}} $ 初始化为0（第06行）。循环的每次迭代都会访问 \( M \) 的第 $ \text{row} $ 行的一个元素和 $N $ 的第 $\text{col} $ 列的一个元素，将这两个元素相乘，并将乘积累加到 $P_{\text{value}} $ 中（第08行）。

让我们首先关注在for循环中访问  M  元素。M  被线性化为一个等价的一维数组，使用行主序。也就是说，M  的行在内存空间中一个接一个地放置，从第0行开始。因此，第1行的开始元素是 $ M[1 \times \text{Width}] $，因为我们需要考虑第0行的所有元素。一般来说，第 $\text{row} $的开始元素是 $M[\text{row} \times \text{Width}] $。由于一行中的所有元素都放置在连续的位置，第 $ \text{row} $ 行的第  k  个元素位于 $ M[\text{row} \times \text{Width} + k] $。这种线性化数组偏移量是我们在图3.11中使用的（第08行）。

现在我们来关注访问 $N $。如图3.11所示，第 $text{col} $ 列的开始元素是第0行的第 $\text{col} $ 个元素，即 $N[\text{col}] $。访问第 $\text{col} $ 列中的下一个元素需要跳过一整行。这是因为同一列的下一个元素是下一行中的同一个元素。因此，第 $ \text{col} $ 列的第  k  个元素是 $ N[k \times \text{Width} + \text{col}] $（第08行）。

当执行退出 for 循环后，所有线程的 P 元素值都在 Pvalue 变量中。然后，每个线程使用一维等效索引表达式 row × Width+col 来写入它的 P 元素（第 10 行）。这种索引模式类似于在 colorToGrayscaleConversion 内核中使用的模式。

![image-20240719152541777](imgs/image-20240719152541777.png)

我们现在用一个小例子来说明矩阵乘法内核的执行。图 3.12 显示了一个 4x4 的 P 矩阵，其中 BLOCK_WIDTH 为 2。尽管这样小的矩阵和块大小并不现实，但它们允许我们把整个例子放入一张图中。P 矩阵被划分为四个瓦片，每个块计算一个瓦片。我们通过创建 2x2 的线程数组块来实现这一点，每个线程计算一个 P 元素。在示例中，块 (0,0) 的线程 (0,0) 计算 P0,0，而块 (1,0) 的线程 (0,0) 计算 P2,0

![image-20240719152600020](imgs/image-20240719152600020.png)

在 matrixMulKernel 中，行和列索引标识了线程要计算的 P 元素。行索引还标识了 M 矩阵的行，而列索引标识了 N 矩阵的列作为线程的输入值。图 3.13 说明了每个线程块中的乘法操作。对于这个小矩阵乘法示例，块 (0,0) 中的线程产生了四个点积。线程块 (0,0) 中的线程 (1,0) 的行和列索引分别是 0 ×0 + 1 = 1 和 0 × 0 + 0 = 0，分别。因此，线程映射到 P1,0 并计算 M 矩阵的第 1 行和 N 矩阵的第 0 列的点积。

让我们通过图 3.11 中块 (0,0) 的线程 (0,0) 的 for 循环的执行来了解。在迭代 0（k=0）期间，row × Width + k = 0 × 4 + 0 = 0 和 k × Width +  col = 0 ×4 + 0 = 0。因此访问的输入元素是 M[0] 和 N[0]，它们是 M0,0 和 N0,0 的一维等效。注意，这些确实是 M 矩阵的第 0 行和 N 矩阵的第 0 列的第 0 个元素。在迭代 1（k=1）期间，row ×Width + k = 0 × 4 +  1 = 1 和 k × Width + col = 1 × 4 +  0 = 4。因此我们访问的是 M[1] 和 N[4]，它们是 M0,1 和 N1,0 的一维等效。这些是 M 矩阵的第 0 行和 N 矩阵的第 0 列的第一个元素。在迭代 2（k=2）期间，row× Width + k = 0  4 + 2 = 2 和 k × Width +  col = 2 ×4 + 0 = 8，结果是 M[2] 和 N[8]。因此访问的元素是 M0,2 和 N2,0 的一维等效。

最后，在迭代 3（k=3）期间，row ×× Width + 1 * k = 0 ×× 4 + 1 * 3 = 3 和 k ×× Width + 1 * col = 3 ×× 4 + 1 * 0 = 12，结果是 M[3] 和 N[12]，M0,3 和 N3,0 的一维等效。

我们现在验证了 for 循环对块 (0,0) 的线程 (0,0) 执行了 M 矩阵的第 0 行和 N 矩阵的第 0 列的内积。循环后，线程写入 P[row ×× Width+col]，这是 P[0]。这是 P0,0 的一维等效，所以块 (0,0) 的线程 (0,0) 成功计算了 M 矩阵的第 0 行和 N 矩阵的第 0 列的内积，并将结果存入 P0,0。

我们将留给读者一个练习，手动执行并验证块 (0,0) 中其他线程的 for 循环，或者在其他块中的线程。 由于网格的大小受到每个网格最大块数和每个块的线程数的限制，matrixMulKernel 能够处理的最大输出矩阵 P 的大小也将受到这些限制的约束。在需要计算大于这个限制的输出矩阵的情况下，可以将输出矩阵划分为子矩阵，这些子矩阵的大小可以被网格覆盖，并使用主机代码为每个子矩阵启动不同的网格。或者，我们可以更改内核代码，以便每个线程计算更多的 P 元素。我们将在本书后面的部分探讨这两种选择

## 3.5 总结

CUDA 网格和块是多维的，最多可以有三个维度。网格和块的多维性对于组织线程映射到多维数据非常有用。内核执行配置参数定义了网格及其块的维度。在 blockIdx 和 threadIdx 中的唯一坐标允许网格的线程识别自己以及它们要处理的数据域。程序员有责任在内核函数中使用这些变量，以便线程能够正确地识别要处理的数据部分。

在访问多维数据时，程序员通常需要将多维索引线性化成一维偏移量。原因是在 C 语言中动态分配的多维数组通常以行主序存储为一维数组。我们使用逐渐增加复杂性的例子来让读者熟悉使用多维网格处理多维数组的机制。这些技能将是理解并行模式及其相关优化技术的基础。

# 4 计算架构和调度

在第1章“引言”中，我们了解到CPU旨在最小化指令执行的延迟，而GPU旨在最大化执行指令的吞吐量。在第2章“异构数据并行计算”和第3章“多维网格和数据”中，我们学习了CUDA编程接口的核心特性，用于创建和调用内核以启动和执行线程。在接下来的三章中，我们将讨论现代GPU的架构，包括计算架构和内存架构，以及由此架构理解而产生的性能优化技术。本章介绍了GPU计算架构的几个方面，这些方面对于CUDA C程序员理解并推理他们的内核代码的性能行为至关重要。我们将首先展示一个高层次的、简化的计算架构视图，并探索灵活的资源分配、块调度和占用率的概念。然后，我们将深入讨论线程调度、延迟容忍、控制分歧和同步（ thread scheduling, latency tolerance, control divergence, synchronization）。我们将以描述可以用来查询GPU中可用资源的API函数以及帮助估计执行内核时GPU占用率的工具来结束本章。在接下来的两章中，我们将介绍GPU内存架构的核心概念和编程考虑。特别是，第5章“内存架构和数据局部性”专注于片上内存架构，第6章“性能考虑”简要介绍片外内存架构，然后详细阐述整个GPU架构的各种性能考虑。掌握了这些概念的CUDA C程序员，将能够编写和理解高性能的并行内核。

## 4.1 现代GPU的架构

![image-20240719162321239](imgs/image-20240719162321239.png)

图 4.1 展示了一个典型的支持CUDA的GPU的高级架构视图，从CUDA C程序员的角度来看。它被组织成一个高度线程化的流式多处理器（SM）数组。每个SM都包含有多个处理单元，称为流处理器或CUDA核心（为了简便，以下简称为核心），如图 4.1 中SM内部的小瓦片所示，它们共享控制逻辑和内存资源。例如，Ampere A100 GPU有108个SM，每个SM有64个核心，整个GPU总共有6912个核心。

SM还配备了不同的片上内存结构，在图 4.1 中统称为“内存”。这些片上内存结构将是第5章“内存架构和数据局部性”的主题。GPU还配备了数GB的片外设备内存，如图 4.1 中所称的“全局内存”。虽然旧的GPU使用图形双倍数据速率同步DRAM，但更近期的GPU，从NVIDIA的Pascal架构开始，可能使用HBM（高带宽内存）或HBM2，它们由与GPU紧密集成在同一个封装中的DRAM（动态随机存取存储器）模块组成。为了简便，我们将在本书的其余部分广泛地将所有这些类型的内存称为DRAM。我们将在第6章“性能考虑”中讨论访问GPU DRAM所涉及的最重要的概念。

## 4.2 块调度

![image-20240719162610792](imgs/image-20240719162610792.png)

当调用内核时，CUDA 运行时系统会启动一个执行内核代码的线程网格。这些线程被逐块分配给 SM。也就是说，一个块中的所有线程同时被分配给同一个 SM。图 4.2 展示了块到 SM 的分配。很可能会有多个块同时被分配给同一个 SM。例如，在图 4.2 中，每个 SM 分配了三个块。然而，块需要保留硬件资源才能执行，因此只能有限数量的块同时被分配给给定的 SM。块数的限制取决于第 4.6 节讨论的各种因素。

由于 SM 的数量有限，同时可以分配给每个 SM 的块的数量也有限，因此在 CUDA 设备中可以同时执行的块的总数也有限制。大多数网格包含的块数远多于这个限制。为确保网格中的所有块都能执行，运行时系统维护了一个需要执行的块的列表，并在之前分配的块完成执行时将新块分配给 SM。

按块将线程分配给 SM 的做法保证了同一个块中的线程会在同一 SM 上同时被调度。这一保证使得同一个块中的线程可以以不同块中的线程无法做到的方式相互交互。 这包括将在第 4.3 节讨论的栅栏同步。它还包括访问位于 SM 上的低延迟共享内存，这将在第 5 章“内存架构和数据局部性”中讨论。

> 不同块中的线程可以通过协作组API执行屏障同步。但是，必须遵守几个重要的限制，以确保所涉及的所有线程确实同时在sm上执行。有兴趣的读者可以参考CUDA C编程指南，以正确使用合作组API

## 4.3 Synchronization and transparent scalability

CUDA 允许同一个块中的线程使用屏障同步函数 `__syncthreads()` 来协调它们的活动。注意“__”由两个“\_”字符组成。当一个线程调用 `__syncthreads()` 时，它会在调用的程序位置被暂停，直到同一个块中的每个线程都到达那个位置。这确保了同一个块中的所有线程在它们中的任何一个可以进入下一个阶段之前，都完成了它们执行的一个阶段。

屏障同步是一种简单且流行的协调并行活动的方法。在现实生活中，我们经常使用屏障同步来协调多个人的并行活动。例如，假设四个朋友开车去购物中心。他们可以分别去不同的商店购买自己的衣服。这是一种并行活动，比他们作为一个团队并依次访问所有感兴趣的商店要高效得多。然而，在他们离开购物中心之前需要进行屏障同步。他们必须等待四个朋友都回到车里才能离开。比其他人早完成的人必须等待那些晚完成的人。如果没有屏障同步，当车离开时，一个或多个人可能会被留在购物中心，这可能会严重损害他们的友谊！

图 4.3 展示了屏障同步的执行。块中有 N 个线程。时间从左到右流逝。一些线程早早地到达了屏障同步语句，一些线程则晚些到达。那些早早到达屏障的线程将等待那些晚到的线程。当最后一个到达屏障时，所有线程都可以继续它们的执行。有了屏障同步，“没有人会被落下”。

![image-20240719162931188](imgs/image-20240719162931188.png)

![image-20240719163218784](imgs/image-20240719163218784.png)

在 CUDA 中，如果存在 `__syncthreads()` 语句，它必须由一个块中的所有线程执行。当 `__syncthreads()` 语句被放置在 if 语句中时，要么块中的所有线程执行包含 `__syncthreads()` 的路径，要么一个也不执行。对于 if-then-else 语句，如果每条路径都有 `__syncthreads()` 语句，要么块中的所有线程执行 then 路径，要么全部执行 else 路径。两个 `__syncthreads()` 是不同的屏障同步点。例如，在图 4.4 中，if 语句中使用了两个 `__syncthreads()`，从第 04 行开始。所有具有偶数 `threadIdx.x` 值的线程执行 then 路径，而其余线程执行 else 路径。第 06 行和第 10 行的 `__syncthreads()` 调用定义了两个不同的屏障。由于不能保证块中的所有线程都执行任一屏障，代码违反了使用 `__syncthreads()` 的规则，将导致未定义的执行行为。通常，不正确使用屏障同步可能导致错误的结果，或者线程永久等待彼此，这被称为死锁。避免这种不适当的屏障同步使用是程序员的责任。

屏障同步对块内的线程施加了执行约束。这些线程应该在彼此接近的时间执行，以避免过长的等待时间。更重要的是，系统需要确保参与屏障同步的所有线程都能够访问必要的资源，最终到达屏障。否则，一个永远无法到达屏障同步点的线程可能会导致死锁。CUDA 运行时系统通过作为一个单元为块中的所有线程分配执行资源来满足这一约束，正如我们在第 4.2 节中看到的那样。不仅一个块中的所有线程必须被分配到同一个 SM，而且它们需要同时被分配到那个 SM。也就是说，只有当运行时系统确保了块中所有线程完成执行所需的所有资源后，一个块才能开始执行。这确保了块中所有线程的时间接近性，并防止了在屏障同步期间出现过度甚至无限期的等待时间。

![image-20240719164434573](imgs/image-20240719164434573.png)

这引导我们来到 CUDA 屏障同步设计中的一个重要权衡。通过不允许不同块中的线程彼此执行屏障同步，CUDA 运行时系统可以以任何顺序执行块，因为它们彼此之间不需要等待。这种灵活性使得可扩展的实现成为可能，如图 4.5 所示。图中的时间从上到下推进。在只有少数执行资源的低成本系统中，可以同时执行少量的块，如图 4.5 左侧所示，一次执行两个块。在具有更多执行资源的高端实现中，可以同时执行许多块，如图 4.5 右侧所示，一次执行四个块。当今的高端 GPU 可以同时执行数百个块。

能够以不同的速度执行相同的应用程序代码，允许根据不同市场细分的成本、功耗和性能要求，生产出不同范围的实现。例如，移动处理器可能以极低的功耗慢速执行应用程序，而桌面处理器可能以更高的速度执行相同的应用程序，同时消耗更多的电能。两者都执行相同的应用程序程序，无需更改代码。能够在具有不同执行资源的不同硬件上执行相同的应用程序代码的能力，被称为透明可扩展性，这减轻了应用程序开发人员的负担，并提高了应用程序的可用性。

## 4.4 Warps and SIMD hardware

我们已经看到，块可以相对于彼此以任何顺序执行，这允许在不同设备之间实现透明可扩展性。然而，我们并没有多谈每个块内线程的执行时机。从概念上讲，我们应该假设块中的线程可以相对于彼此以任何顺序执行。在具有阶段的算法中，每当我们希望确保所有线程在开始下一个阶段之前都完成了它们执行的前一个阶段时，应该使用屏障同步。执行内核的正确性不应该依赖于任何假设，即某些线程会在没有使用屏障同步的情况下彼此同步执行。

CUDA GPU中的线程调度是一个硬件实现概念，因此必须在特定硬件实现的背景下讨论。在迄今为止的大多数实现中，一旦一个块被分配给SM，它就被进一步划分为32线程单位，称为warp。warp的大小是实现特定的，并且在未来几代GPU中可能会有所不同。了解warp可以帮助理解并优化特定代CUDA设备上CUDA应用程序的性能。

![image-20240719170103162](imgs/image-20240719170103162.png)

Warp是SM中线程调度的单位。图4.6显示了一个实现中块被划分为warp的情况。在这个例子中，有三个块——块1、块2和块3——都被分配给了一个SM。这三个块都进一步被划分为warp以进行调度。每个warp由连续的threadIdx值的32个线程组成：线程0到31形成第一个warp，线程32到63形成第二个warp，以此类推。我们可以计算给定块大小和每个SM分配的块数时，SM中驻留的warp数量。在这个例子中，如果每个块有256个线程，我们可以确定每个块有256/32或8个warp。在SM中有3个块，我们有8 * 3 = 24个warp在SM中。

块是基于线程索引被划分为warp的。如果一个块被组织成一个一维数组，也就是说，只使用threadIdx.x，那么划分是直接的。warp内的threadIdx.x值是连续且递增的。对于32大小的warp，warp 0从线程0开始，到线程31结束，warp 1从线程32开始，到线程63结束，以此类推。一般来说，warp n从线程32 * n开始，到线程32 * (n+1) - 1结束。

对于大小不是32的倍数的块，最后一个warp将用不活跃的线程填充，以补足32个线程位置。例如，如果一个块有48个线程，它将被划分为两个warp，第二个warp将用16个不活跃的线程填充。

对于由多维线程组成的块，这些维度将在划分为warp之前被投影到一个线性化的行主序布局中。线性布局是通过将具有较大y和z坐标的行放在较小坐标的行后面来确定的。也就是说，如果一个块由两个维度的线程组成，将通过将所有threadIdx.y为1的线程放在threadIdx.y为0的线程后面来形成线性布局。threadIdx.y为2的线程将被放置在threadIdx.y为1的线程后面，以此类推。具有相同threadIdx.y值的线程将按增加的threadIdx.x顺序放置在连续的位置。

![image-20240719170411488](imgs/image-20240719170411488.png)

图 4.7 展示了将二维块中的线程放置到线性布局中的例子。上图显示了块的二维视图。读者应该能够认出它与二维数组的行主序布局的相似性。每个线程显示为 Ty,x，其中 x 是 threadIdx.x，y 是 threadIdx.y。图 4.7 的下半部分显示了块的线性化视图。前四个线程是那些 threadIdx.y 值为 0 的线程；它们按照递增的 threadIdx.x 值排序。接下来的四个线程是那些 threadIdx.y 值为 1 的线程。它们也按照递增的 threadIdx.x 值放置。在这个例子中，所有 16 个线程形成了半个 warp。warp 将用另外 16 个线程填充，以完成一个 32 线程的 warp。想象一下一个有 8x8 线程的二维块。这 64 个线程将形成两个 warp。第一个 warp 从 T0,0 开始，以 T3,7 结束。第二个 warp 从 T4,0 开始，以 T7,7 结束。对于读者来说，作为练习画出这个图将会很有帮助。

对于三维块，我们首先将所有 threadIdx.z 值为 0 的线程放置到线性顺序中。这些线程被当作一个二维块来对待，如图 4.7 所示。所有 threadIdx.z 值为 1 的线程随后将被放置到线性顺序中，以此类推。例如，对于一个三维 2x8x4 块（x 维有四个线程，y 维有八个线程，z 维有两个线程），这 64 个线程将被划分为两个 warp，第一个 warp 中有 T0,0,0 到 T0,7,3，第二个 warp 中有 T1,0,0 到 T1,7,3。

![image-20240719170651807](imgs/image-20240719170651807.png)

流式多处理器（SM）被设计为按照单指令多数据（SIMD）模型执行所有warp中的线程。也就是说，在任何给定的时刻，一个指令被取出并被warp中的所有线程执行（见“warp和SIMD硬件”边栏）。图4.8显示了SM中的核心是如何被分组到处理块中，其中每8个核心形成一个处理块，并共享一个指令获取/调度单元。以一个真实的例子为例，Ampere A100 SM拥有64个核心，被组织成四个处理块，每个处理块有16个核心。同一个warp中的线程被分配到同一个处理块，该处理块为warp获取指令并同时为warp中的所有线程执行它。这些线程将相同的指令应用于数据的不同部分。由于SIMD硬件有效地限制了warp中的所有线程在任何时刻执行相同的指令，因此一个warp的执行行为通常被称为单指令多线程。

SIMD的优势在于控制硬件（如指令获取/调度单元）的成本可以在许多执行单元之间共享。这种设计选择允许更小比例的硬件用于控制，更大比例的硬件用于提高算术吞吐量。我们预计在可预见的未来，warp划分将保持为一种流行的实现技术。然而，warp的大小可以从一种实现变化到另一种实现。到目前为止，所有CUDA设备都使用了相似的warp配置，其中每个warp由32个线程组成。

> warp和SIMD硬件
>
> 在他的开创性1945年报告中，约翰·冯·诺依曼描述了一种构建电子计算机的模型，该模型基于先驱EDVAC计算机的设计。这个模型，现在通常被称为“冯·诺依曼模型”，几乎成为了所有现代计算机的基础蓝图。
>
> 冯·诺依曼模型在下图中进行了说明。计算机有一个I/O（输入/输出），允许程序和数据被提供给系统并从系统中生成。要执行程序，计算机首先将程序及其数据输入到内存中。
>
> ![image-20240719171307586](imgs/image-20240719171307586.png)
>
> 冯·诺依曼模型的核心是程序由一系列指令组成。控制单元维护一个程序计数器（PC），其中包含下一条要执行指令的内存地址。在每个“指令周期”中，控制单元使用PC将指令取出到指令寄存器（IR）。然后检查指令位以确定计算机的所有组件要采取的动作。这就是为什么该模型也被称为“存储程序”模型，这意味着用户可以通过将不同的程序存储到其内存中来改变计算机的行为。
>
> 执行线程作为warp的动机在以下修改后的冯·诺依曼模型中得到了说明，该模型经过调整以反映GPU设计。处理器，对应于图4.8中的处理块，只有一个控制单元来获取和调度指令。相同的控制信号（在图4.8中从控制单元到处理单元的箭头）发送到多个处理单元，每个处理单元对应于SM中的一个核心，每个核心执行一个warp中的线程。
>
> ![image-20240719171431566](imgs/image-20240719171431566.png)
>
> 由于所有处理单元都由控制单元的指令寄存器（IR）中的相同指令控制，它们的执行差异是由于寄存器文件中不同的数据操作数值造成的。在处理器设计中，这被称为单指令多数据（SIMD）。例如，尽管所有处理单元（核心）都受到指令的控制，如 add r1, r2, r3，但在不同的处理单元中，r2 和 r3 的内容是不同的。
>
> 现代处理器的控制单元非常复杂，包括用于获取指令的复杂逻辑和对指令缓存的访问端口。让多个处理单元共享一个控制单元可以显著降低硬件制造成本和功耗。

## 4.5 Control divergence

SIMD 在所有线程处理它们各自的数据时遵循相同的执行路径，更正式地说，是控制流时，效果最佳。例如，在 if-else 结构中，当一个 warp 中的所有线程执行 if 路径或所有线程执行 else 路径时，执行效果良好。然而，当 warp 内的线程采取不同的控制流路径时，SIMD 硬件将多次通过这些路径，每条路径一次。例如，在 if-else 结构中，如果一些线程遵循 if 路径，而其他线程遵循 else 路径，硬件将进行两次传递。一次传递执行遵循 if 路径的线程，另一次传递执行遵循 else 路径的线程。在每次传递中，遵循其他路径的线程不允许生效。

当同一个 warp 中的线程遵循不同的执行路径时，我们说这些线程表现出控制分歧，即它们在执行中发生了分歧。处理分歧的 warp 多通道方法扩展了 SIMD 硬件实现 CUDA 线程完整语义的能力。虽然硬件为 warp 中的所有线程执行相同的指令，但它选择性地让这些线程只在对应于它们所采取路径的传递中生效，使每个线程看起来都采取了自己的控制流路径。这在利用 SIMD 硬件成本降低的同时保持了线程的独立性。然而，分歧的代价是硬件需要额外的传递次数，以允许 warp 中的不同线程做出自己的决策，以及在每次传递中被不活跃线程消耗的执行资源。

![image-20240719171923762](imgs/image-20240719171923762.png)

图 4.9 展示了一个 warp 如何执行一个分歧的 if-else 语句。在这个例子中，当包含线程 0 到 31 的 warp 到达 if-else 语句时，线程 0 到 23 选择了 then 路径，而线程 24 到 31 选择了 else 路径。在这种情况下，warp 将执行一次代码传递，其中线程 0 到 23 执行 A，而线程 24 到 31 处于非活跃状态。warp 还将执行另一次代码传递，其中线程 24 到 31 执行 B，而线程 0 到 23 处于非活跃状态。然后，warp 中的线程重新汇聚并执行 C。在帕斯卡架构及之前的架构中，这些传递是顺序执行的，意味着一次传递执行完成后再执行另一次传递。从伏特架构开始，这些传递可能会并发执行，这意味着一个传递的执行可能与另一个传递的执行交错进行。这个特性被称为独立线程调度。感兴趣的读者可以参考伏特 V100 架构的白皮书（NVIDIA，2017年）以获取详细信息。

![image-20240719172210174](imgs/image-20240719172210174.png)

分歧也可能在其他控制流结构中产生。图 4.10 展示了一个 warp 如何执行一个分歧的 for 循环。在这个例子中，每个线程执行不同数量的循环迭代，这些迭代在四次和八次之间变化。在前四次迭代中，所有线程都是活跃的并执行 A。在剩余的迭代中，一些线程执行 A，而其他线程则处于非活跃状态，因为它们已经完成了它们的迭代。

人们可以通过检查其决策条件来确定一个控制结构是否可能导致线程分歧。如果决策条件基于 threadIdx 值，那么控制语句就可能引起线程分歧。例如，语句 if(threadIdx.x >2) {...} 会导致块中的第一个 warp 的线程遵循两个不同的控制流路径。线程 0、1 和 2 遵循的路径与线程 3、4、5 等不同。同样，如果循环条件基于线程索引值，循环也可能引起线程分歧。

使用具有线程控制分歧的控制结构的一个普遍原因是在将线程映射到数据时处理边界条件。这通常是因为总线程数需要是线程块大小的倍数，而数据的大小可以是任意数字。从我们在第 2 章“异构数据并行计算”中的向量加法内核开始，我们在 addVecKernel 中就有一个 if(i < n) 语句。这是因为不是所有的向量长度都可以表示为块大小的倍数。例如，假设向量长度是 1003，我们选择了 64 作为块大小。人们需要启动 16 个线程块来处理所有的 1003 个向量元素。然而，这 16 个线程块将有 1024 个线程。我们需要禁用线程块 15 中的最后 21 个线程进行工作，这些工作不是原始程序所期望的或不允许的。请记住，这 16 个块被划分为 32 个 warp。只有最后一个 warp（即最后一个块中的第二个 warp）将有控制分歧。

请注意，随着正在处理的向量大小的增加，控制分歧对性能的影响会减少。对于向量长度为 100，四个 warp 中的一个将有控制分歧，这可能会对性能产生显著影响。对于向量大小为 1000，只有 32 个 warp 中的一个将有控制分歧。也就是说，控制分歧将只影响大约 3% 的执行时间。即使它使 warp 的执行时间翻倍，对总执行时间的净影响也将是大约 3%。显然，如果向量长度为 10,000 或更长，只有 313 个 warp 中的一个将有控制分歧。控制分歧的影响将远小于 1%！

对于二维数据，比如第3章“多维网格和数据”中的彩色到灰度的转换示例，if语句也用来处理处于数据边缘的线程的边界条件。在图3.2中，为了处理62x76的图像，我们使用了20个4x5的二维块，每个块由16x16的线程组成。每个块将被划分为8个warp，每个warp由块的两行组成。总共涉及160个warp（每个块8个warp）。为了分析控制分歧的影响，请参考图3.5。在区域1的12个块中，没有一个warp会有控制分歧。区域1有12x8=96个warp。对于区域2，所有的24个warp都会有控制分歧。对于区域3，所有的底部warp都被映射到完全在图像外的数据上。因此，它们都不会通过if条件。读者应该验证，如果图像在垂直维度上有一个像素的奇数，这些warp就会有控制分歧。

在区域4，前7个warp将有控制分歧，但最后一个warp不会。总的来说，160个warp中有31个会有控制分歧。

再次强调，随着水平维度上像素数量的增加，控制分歧对性能的影响会减少。例如，如果我们处理一个200x150的图片，使用16x16的块，将总共有130=13x10个线程块或1040个warp。区域1到4的warp数量将是864（12x9x8）、72（9x8）、96（12x8）和8（1x8）。其中只有80个warp会有控制分歧。因此，控制分歧对性能的影响将小于8%。显然，如果我们处理一个水平维度上超过1000像素的实际图片，控制分歧对性能的影响将小于2%。

控制分歧的一个重要含义是，我们不能假设一个warp中的所有线程具有相同的执行时间。因此，如果一个warp中的所有线程必须完成它们执行的一个阶段才能继续，就必须使用屏障同步机制，如__syncwarp()，以确保正确性。

## 4.6 Warp scheduling and latency tolerance

当线程被分配到流式多处理器（SM）时，通常分配给SM的线程数量多于SM中的核心数量。也就是说，每个SM在任何给定时刻只有足够的执行单元来执行分配给它的所有线程的一个子集。

在早期的GPU设计中，每个SM在任何给定瞬间只能为单个warp执行一个指令。在更近期的设计中，每个SM在任何给定瞬间可以为少量warp执行指令。在这两种情况下，硬件只能在任何时刻为SM中的所有warp的一个子集执行指令。一个合理的问题是，如果SM在任何瞬间只能执行它们的一个子集，为什么我们需要将这么多warp分配给SM？答案是，这就是GPU处理长延迟操作（如全局内存访问）的方式。

当一个warp要执行的指令需要等待之前启动的长延迟操作的结果时，该warp不会被选中执行。相反，另一个不再等待之前指令结果的驻留warp将被选中执行。如果有多个warp准备好执行，将使用优先级机制来选择一个执行。用其他线程的工作来填补某些线程操作的延迟时间的机制通常被称为“延迟容忍”或“延迟隐藏”（见“延迟容忍”侧边栏）。

> 延迟容忍在许多日常情况中都是必需的。例如，在邮局，每个想要邮寄包裹的人都应该在到达服务柜台之前填写好所有表格和标签。然而，正如我们都经历过的，有些人会等待服务台职员告诉他们需要填写哪些表格以及如何填写。
>
> 当服务台前有一条长队时，最大化服务台职员的生产力是很重要的。让一个人在职员面前填写表格，而其他人都在等待，并不是一个好方法。职员应该在那个人填写表格的同时，帮助下一位等待的顾客。这些其他顾客已经“准备好了”，不应该因为需要更多时间填写表格的顾客而被阻塞。
>
> 这就是为什么一个好的职员会礼貌地要求第一位顾客先到一边填写表格，而职员则为其他顾客服务。在大多数情况下，第一位顾客不会排到队伍的末尾，而是在他或她填写完表格并且职员服务完当前顾客后立即得到服务。
>
> 我们可以将这些邮局顾客视为warp，将职员视为硬件执行单元。需要填写表格的顾客对应于一个warp，其继续执行依赖于长延迟操作。

warp调度也用于容忍其他类型的操作延迟，例如流水线浮点运算和分支指令。有足够的warp存在时，硬件在任何给定时刻都很可能找到一个可执行的warp，从而在某些warp的指令等待这些长延迟操作的结果时，充分利用执行硬件。选择准备好执行的warp不会在执行时间轴中引入任何空闲或浪费的时间，这被称为零开销线程调度（见“线程、上下文切换和零开销调度”侧边栏）。通过warp调度，warp指令的长时间等待通过执行其他warp的指令被“隐藏”。这种容忍长操作延迟的能力是GPU不像CPU那样将大量芯片区域专门用于缓存内存和分支预测机制的主要原因。因此，GPU可以将更多的芯片区域专门用于浮点执行和内存访问通道资源。

> 线程、上下文切换和零开销调度
>
> 基于冯·诺依曼模型，我们准备更深入地理解线程是如何实现的。在现代计算机中，线程是程序以及在冯·诺依曼处理器上执行程序的状态。回想一下，一个线程由程序的代码、正在执行的代码中的指令以及其变量和数据结构的值组成。
>
> 在基于冯·诺依曼模型的计算机中，程序的代码存储在内存中。程序计数器（PC）跟踪正在执行的程序指令的地址。指令寄存器（IR）保存正在执行的指令。寄存器和内存保存变量和数据结构的值。
>
> 现代处理器被设计为允许上下文切换，多个线程可以通过轮流取得进展来共享处理器。通过仔细保存和恢复PC值以及寄存器和内存的内容，我们可以暂停一个线程的执行，并在稍后正确地恢复该线程的执行。然而，在这些处理器中，在上下文切换过程中保存和恢复寄存器内容可能会增加显著的开销，以增加执行时间。
>
> 零开销调度指的是GPU能够使需要等待长延迟指令结果的warp进入睡眠状态，并激活准备好的warp，而不在处理单元中引入任何额外的空闲周期。传统的CPU会因为切换一个线程到另一个线程的执行而产生这样的空闲周期，这需要将执行状态（如退出线程的寄存器内容）保存到内存中，并从内存中加载进入线程的执行状态。GPU的流式多处理器（SM）通过将所有分配的warp的执行状态保存在硬件寄存器中来实现零开销调度，因此在从一个warp切换到另一个warp时无需保存和恢复状态。

为了使延迟容忍有效，希望一个流式多处理器（SM）被分配的线程数量远远多于其执行资源可以同时支持的数量，以最大化在任何给定时刻找到一个准备好执行的warp的机会。例如，在Ampere A100 GPU中，一个SM有64个核心，但可以同时被分配多达2048个线程。因此，SM可以被分配多达其核心在任何给定时钟周期可以支持的32倍的线程。线程对SM的这种过度订阅对于延迟容忍至关重要。它增加了在当前执行的warp遇到长延迟操作时找到另一个warp执行的机会。

## 4.7 资源的划分和占用情况

为了使延迟容忍有效，通常希望将许多warp分配给一个流式多处理器（SM），以便容忍长延迟操作。然而，可能并非总是能够将SM支持的最大warp数量分配给SM。分配给SM的warp数量与它支持的最大数量之比被称为占用率。为了理解是什么可能阻止SM达到最大占用率，首先了解SM资源是如何划分的非常重要。

SM中的执行资源包括寄存器、共享内存（在第5章“内存架构和数据局部性”中讨论）、线程块slots和线程slots。这些资源在线程之间动态划分，以支持它们的执行。例如，Ampere A100 GPU每个SM最多可以支持32个块，每个SM 64个warp（2048个线程），每个块最多1024个线程。如果以1024个线程的块大小启动网格（允许的最大值），则每个SM中的2048个线程slots被划分并分配给2个块。在这种情况下，每个SM最多可以容纳2个块。同样，如果以512、256、128或64个线程的块大小启动网格，则2048个线程slots被划分并分别分配给4、8、16或32个块。

这种动态划分线程slots的能力使SM变得多功能。它们可以执行许多每个只有少数线程的块，或者执行每个有许多线程的少数块。这种动态划分可以与固定划分方法形成对比，固定划分方法中，每个块将获得固定数量的资源，而不管其实际需求如何。当一个块所需的线程少于固定划分支持的数量时，固定划分会导致线程slots浪费，并且无法支持需要多于固定划分的线程slots的块。

动态资源分配可能导致资源限制之间发生微妙的相互作用，这可能导致资源利用不足。这种相互作用可能发生在块slots和线程slots之间。以Ampere A100为例，我们可以看到块大小可以从1024变化到64，分别导致每个SM有2到32个块。在所有这些情况下，分配给SM的线程总数是2048，这最大化了占用率。然而，考虑一下每个块有32个线程的情况。在这种情况下，2048个线程slots需要被划分并分配给64个块。但是，Volta SM一次只能支持32个块slots。这意味着只有1024个线程slots会被利用，即每个块有32个线程的32个块。在这种情况下，占用率是（1024个分配线程）/（2048个最大线程）= 50%。因此，为了充分利用线程插槽并实现最大占用率，每个块至少需要有64个线程。

另一个可能对占用率产生负面影响的情况是，每个块的最大线程数不能被块大小整除。以Ampere A100为例，我们可以看到每个SM最多可以支持2048个线程。然而，如果选择768个线程的块大小，SM只能容纳2个线程块（1536个线程），留下512个线程slots未被利用。在这种情况下，既没有达到每个SM的最大线程数，也没有达到每个SM的最大块数。在这种情况下，占用率是（1536个分配线程）/（2048个最大线程）= 75%。

动态资源分配可能不会考虑其他资源限制的影响，比如寄存器和共享内存。我们将在第5章“内存架构和数据局部性”中看到，CUDA内核中声明的自动变量被放置在寄存器中。一些内核可能使用许多自动变量，而其他内核可能使用较少的自动变量。因此，人们应该预期一些内核每个线程需要许多寄存器，而一些则需要较少。通过在SM中动态划分寄存器，如果它们每个线程需要较少的寄存器，SM可以容纳许多块；如果它们需要更多的寄存器，则可以容纳较少的块。

然而，人们确实需要意识到寄存器资源限制可能对占用率产生的影响。例如，Ampere A100 GPU每个SM允许最多65536个寄存器。为了实现完全的占用率，每个SM需要足够2048个线程的寄存器，这意味着每个线程不应该使用超过(65536寄存器)/(2048线程)=每个线程32个寄存器。例如，如果一个内核每个线程使用64个寄存器，那么在65536个寄存器下可以支持的最大线程数是1024个线程。在这种情况下，无论块大小设置为多少，内核都无法以完全的占用率运行。相反，占用率最多为50%。在某些情况下，编译器可能会执行寄存器溢出操作，以减少每个线程的寄存器需求，从而提高占用率的水平。然而，这通常以增加线程访问内存中溢出寄存器值的执行时间为代价，可能会导致网格的总执行时间增加。对于共享内存资源的类似分析在第5章“内存架构和数据局部性”中进行。

假设程序员实现了一个每个线程使用31个寄存器的内核，并将其配置为每个块512个线程。在这种情况下，SM将有(2048线程)/(512线程/块)=4个块同时运行。这些线程总共将使用(2048线程)x(每个线程31个寄存器)=63488个寄存器，这少于65536个寄存器限制。现在假设程序员在内核中声明了另外两个自动变量，将每个线程使用的寄存器数量增加到33。2048个线程现在需要的寄存器数量是67584个，这超过了寄存器限制。CUDA运行时系统可能会通过为每个SM分配3个块而不是4个来处理这种情况，从而将所需的寄存器数量减少到50688个寄存器。

然而，这将SM上运行的线程数量从2048减少到1536；也就是说，通过使用两个额外的自动变量，程序的占用率从100%降低到75%。这有时被称为“性能悬崖”，在这种情况下，资源使用的轻微增加可能导致并行性和性能实现的显著降低（Ryoo等人，2008）。

读者应该清楚，所有动态划分资源的限制以复杂的方式相互作用。准确确定每个SM中运行的线程数量可能是困难的。读者可以参考CUDA占用率计算器（CUDA Occupancy Calculator，Web），这是一个可下载的电子表格，它计算了给定内核资源使用情况下，特定设备实现中每个SM上实际运行的线程数量。

## 4.8 查询设备属性

关于SM资源分配的讨论引出了一个重要的问题：我们如何了解特定设备可用的资源量？当一个CUDA应用程序在系统上执行时，它如何了解设备中的SM数量以及可以分配给每个SM的块和线程的数量？同样的问题也适用于其他类型的资源，有些我们还没有讨论。通常，许多现代应用程序被设计为在各种硬件系统上执行。应用程序通常需要查询底层硬件的可用资源和功能，以便利用功能更强的系统，同时补偿功能较弱的系统（见“资源和功能查询”侧边栏）。

> 资源和功能查询
> 在日常生活中，我们经常查询环境中的资源和功能。例如，当我们预订酒店时，我们可以检查酒店房间提供的设施。如果房间配有吹风机，我们就不需要自带。大多数美国的酒店房间都配备有吹风机，而在其他地区的许多酒店则没有。
>
> 一些亚洲和欧洲的酒店提供牙膏甚至牙刷，而大多数美国的酒店则不提供。许多美国的酒店同时提供洗发水和护发素，而其他大陆的酒店通常只提供洗发水。
>
> 如果房间配有微波炉和冰箱，我们可以将晚餐的剩菜带走，并期待第二天继续食用。如果酒店有游泳池，我们可以带上泳衣，在商务会议后游泳。如果酒店没有游泳池但有健身房，我们可以带上跑鞋和运动服去锻炼。一些高端的亚洲酒店甚至提供运动服装！
>
> 这些酒店便利设施是酒店的属性，或资源和功能的一部分。经验丰富的旅行者会在酒店网站上查看这些属性，选择最符合他们需求的酒店，并更高效、更有效地打包行李。

CUDA 设备的每个流式多处理器（SM）中的资源数量是作为设备的计算能力的指定部分。通常来说，计算能力级别越高，每个SM中可用的资源就越多。GPU的计算能力通常随着代际的更迭而增加。Ampere A100 GPU的计算能力为8.0。
在CUDA C中，主机代码有一个内置机制来查询系统中可用设备的属性。CUDA运行时系统（设备驱动程序）有一个API函数cudaGetDeviceCount，它返回系统中可用的CUDA设备的数量。主机代码可以通过使用以下语句来找出系统中可用的CUDA设备数量：

![image-20240722155032074](imgs/image-20240722155032074.png)

现代PC系统通常有两个或更多的CUDA设备，这可能并不明显。这是因为许多PC系统配备了一个或多个“集成”GPU。这些GPU是默认的图形单元，提供基本的功能和硬件资源，以执行现代基于窗口的用户界面所需的最小图形功能。大多数CUDA应用程序在这些集成设备上的性能不会很好。这将是主机代码遍历所有可用设备，查询它们的资源和能力，并选择具有足够资源以令人满意的性能执行应用程序的原因。
CUDA运行时将系统中所有可用的设备从0编号到devCount-1。它提供了一个API函数cudaGetDeviceProperties，该函数返回作为参数给出的设备编号的属性。例如，我们可以使用以下语句在主机代码中遍历所有可用的设备并查询它们的属性：

![image-20240722155148544](imgs/image-20240722155148544.png)

内置类型 cudaDeviceProp 是一个 C 结构体类型，其字段代表 CUDA 设备的属性。读者可以参考 CUDA C 编程指南了解该类型的所有字段。我们将讨论其中一些与线程执行资源分配特别相关的字段。我们假设属性是通过 cudaGetDeviceProperties 函数设置的 devProp 变量返回的。如果读者选择为变量命名不同，显然需要在以下讨论中替换为适当的变量名。
顾名思义，字段 devProp.maxThreadsPerBlock 给出了查询设备中每个块允许的最大线程数。一些设备每个块允许多达 1024 个线程，其他设备可能允许较少。未来设备甚至可能允许每个块超过 1024 个线程。因此，查询可用设备并确定哪些设备允许每个块有足够数量的线程，这对应用程序来说是一个很好的主意。
设备中的 SM 数量在 devProp.multiProcessorCount 中给出。如果应用程序需要许多 SM 才能达到令人满意的性能，那么绝对应该检查预期设备的这个属性。此外，设备的时钟频率在 devProp.clockRate 中。时钟频率和 SM 数量的组合为设备的最大硬件执行吞吐量提供了很好的指示。
主机代码可以在字段 devProp.maxThreadsDim[0]（对于 x 维度）、devProp.maxThreadsDim[1]（对于 y 维度）和 devProp.maxThreadsDim[2]（对于 z 维度）中找到每个块允许的每个维度上的最大线程数。使用这些信息的一个例子是，自动调优系统在评估底层硬件的最佳性能块尺寸时设置块尺寸的范围。类似地，它可以在 devProp.maxGridSize 中找到每个网格允许的每个维度上的最大块数 [0]（对于 x 维度）、devProp.maxGridSize[1]（对于 y 维度）和 devProp.maxGridSize[2]（对于 z 维度）。这种信息的典型用途是确定网格是否有足够的线程来处理整个数据集，或者是否需要某种迭代方法。
字段 devProp.regsPerBlock 给出了每个 SM 中可用的寄存器数量。这个字段在确定内核能否在特定设备上实现最大占用率，或者是否会因其寄存器使用而受到限制时非常有用。请注意，该字段的名称有点误导。对于大多数计算能力级别，一个块可以使用的最大寄存器数量确实是 SM 中可用的总寄存器数量。然而，对于一些计算能力级别，一个块可以使用的最大寄存器数量小于 SM 上可用的总数。
我们还讨论了 warp 大小取决于硬件。warp 的大小可以从 devProp.warpSize 字段中获得。
cudaDeviceProp 类型中还有很多其他字段。随着我们介绍它们旨在反映的概念和特性，我们将在本书中讨论它们。

## 4.9 总结

GPU 被组织成 SM，它们由多个处理块组成，这些处理块共享控制逻辑和内存资源。当一个网格被启动时，它的块被任意顺序地分配给 SM，从而实现了 CUDA 应用程序的透明可扩展性。透明可扩展性带来了一个限制：不同块中的线程不能相互同步。
线程被分配到 SM 进行执行，这是基于块来块去的。一旦一个块被分配给了一个 SM，它会被进一步划分为 warp。warp 中的线程按照 SIMD 模型执行。如果同一 warp 中的线程通过采取不同的执行路径而发散，处理块会以每个线程只在与其路径相对应的通过中活跃的方式执行这些路径。
一个 SM 可能被分配了比它能够同时执行的线程多得多的线程。在任何时候，SM 只执行其居民 warp 的一小部分的指令。这允许其他 warp 在等待长延迟操作时不会减慢大量处理单元的整体执行吞吐量。分配给 SM 的线程数与它能够支持的最大线程数的比率被称为占用率。SM 的占用率越高，它就越能隐藏长延迟操作。
每个 CUDA 设备对每个 SM 可用的资源量施加了可能不同的限制。例如，每个 CUDA 设备对每个 SM 能够容纳的块数、线程数、寄存器数和其他资源的数量都有限制。对于每个内核，这些资源限制中的一个或多个可能成为占用率的限制因素。CUDA C 为程序员提供了在运行时查询 GPU 可用资源的能力。

# 5 内存架构和数据局部性

到目前为止，我们已经学会了如何编写 CUDA 内核函数，以及如何配置和协调由大量线程执行。我们还研究了当前 GPU 硬件的计算架构以及线程如何被调度在该硬件上执行。在这一章中，我们将重点关注 GPU 的片上内存架构，并开始研究如何组织和定位数据，以便由大量线程高效访问。到目前为止我们研究的 CUDA 内核可能只能实现潜在硬件速度的一小部分。这种性能不佳是因为全局内存通常使用片外 DRAM 实现，往往具有较长的访问延迟（数百个时钟周期）和有限的访问带宽。尽管理论上拥有大量线程可供执行可以容忍长时间的内存访问延迟，但人们很容易遇到全局内存访问路径上的交通拥堵，使得除了极少数线程之外的所有线程都无法取得进展，从而使流式多处理器（SM）中的一些核心处于空闲状态。为了避免这种拥堵，GPU 提供了一些额外的片上内存资源来访问数据，可以消除大部分对全局内存的流量。在这一章中，我们将学习使用不同类型的内存来提升 CUDA 内核的执行性能。

## 5.1 内存访问效率的重要性

内存访问效率的影响可以通过计算图 3.11 中矩阵乘法内核代码最常执行部分的预期性能水平来说明，该代码的部分副本在图 5.1 中复制。内核在执行时间方面最重要的部分是执行 M 行和 N 列的点积的 for 循环。

在循环的每次迭代中，进行两次全局内存访问，用于一次浮点乘法和一次浮点加法。全局内存访问从 M 和 N 数组中获取元素。浮点乘法操作将这两个元素相乘，浮点加操作将乘积累加到 Pvalue 中。因此，浮点运算（FLOP）与从全局内存访问的字节（B）的比率是 2 FLOP 到 8 B，或 0.25 FLOP/B。我们将这个比率称为计算到全局内存访问比率，定义为程序中一个区域内每个字节从全局内存访问时执行的 FLOP 数量。这个比率有时在文献中也被称为算术强度或计算强度。

计算到全局内存访问比率对 CUDA 内核的性能有重大影响。例如，Ampere A100 GPU 的峰值全局内存带宽为 1555 GB/秒。由于矩阵乘法内核执行 0.25 OP/B，全局内存带宽限制了内核可以执行的单精度 FLOPs 的吞吐量为每秒 389 亿次（GFLOPS），这是通过将 1555 GB/秒乘以 0.25 FLOP/B 得到的。然而，389 GFLOPS 仅占 A100 GPU 峰值单精度操作吞吐量的 2%，后者为 19500 GFLOPS。A100 还配备了称为张量核心的特殊单元，这些单元对加速矩阵乘法操作非常有用。如果考虑到 A100 的张量核心峰值单精度浮点吞吐量为 156000 GFLOPS，那么 389 GFLOPS 仅占峰值的 0.25%。因此，矩阵乘法内核的执行严重受到数据从内存传送到 GPU 核心的速率的限制。我们将执行速度受到内存带宽限制的程序称为内存绑定程序。

![image-20240722161902024](imgs/image-20240722161902024.png)

> The Roofline Model
>
> Roofline 模型是一种可视化模型，用于评估应用程序相对于其所运行硬件的限制所达到的性能。Roofline 模型的基本示例如下所示。
>
> ![image-20240722162105249](imgs/image-20240722162105249.png)
>
> 在 x 轴上，我们有算术或计算强度，以 FLOP/B 为单位进行测量。它反映了应用程序为加载的每一字节数据所做的工作量。在 y 轴上，我们有计算吞吐量，以 GFLOPS 为单位进行测量。图表内的两条线反映了硬件的限制。水平线由硬件能够维持的峰值计算吞吐量（GFLOPS）决定。从原点开始具有正斜率的线由硬件能够维持的峰值内存带宽决定。图表中的一个点代表一个应用程序，其操作强度在 x 轴上，它实现的计算吞吐量在 y 轴上。当然，这些点会在两条线下面，因为它们不能实现比硬件峰值更高的吞吐量。
>
> 一个点相对于这两条线的位置告诉我们应用程序的效率。靠近这两条线的点表明应用程序正在有效使用内存带宽或计算单元，而远低于这些线的应用程序表明资源使用效率低下。这两条线的交点代表了计算强度值，在该值下，应用程序从内存限制转变为计算限制。计算强度较低的应用程序是内存绑定的，无法实现峰值吞吐量，因为它们受到内存带宽的限制。计算强度较高的应用程序是计算绑定的，不受内存带宽的限制。
>
> 例如，点 A1 和 A2 都代表内存绑定的应用程序，而 A3 代表一个计算绑定的应用程序。A1 有效使用资源，并且运行接近峰值内存带宽，而 A2 则不是。对于 A2，可能还有改进内存带宽利用率以提高吞吐量的额外优化空间。然而，对于 A1，提高吞吐量的唯一方法是增加应用程序的计算强度。

